"""
Agentic Coderì˜ ì—ì´ì „íŠ¸ êµ¬í˜„

3ê°œì˜ ì—ì´ì „íŠ¸:
1. Specification Writer Agent - ìš”ì²­ì‚¬í•­ â†’ ëª…ì„¸ì„œ ì‘ì„± (API ì‹œê·¸ë‹ˆì²˜ ì¶”ì¶œ ë° ì£¼ì…)
2. Code Generator Agent - ëª…ì„¸ì„œ â†’ ì½”ë“œ ì‘ì„±
3. Static Reviewer Agent - ì‘ì„±ëœ ì½”ë“œ â†’ ì •ì  ë¦¬ë·°
"""

import os
from typing import Dict, Any
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate

from .schemas import (
    AgenticCoderState,
    Specification,
    SingleFileGeneration,
    StaticReviewResult,
    CodeIssue,
    OrchestratorDecision,
    TokenUsage
)


def get_llm(model: str = "gpt-5-mini", is_openai: bool = True):
    """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    if is_openai:
        return ChatOpenAI(model=model)
    else:
        return ChatGoogleGenerativeAI(model=model)

def extract_token_usage(result, step_name: str) -> TokenUsage:
    """
    LLM ì‘ë‹µì—ì„œ ì§ì ‘ í† í° ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        result: LLM ì‘ë‹µ ê°ì²´ (AIMessage ë“±)
        step_name: ë‹¨ê³„ ì´ë¦„
    
    Returns:
        TokenUsage ê°ì²´
    """
    input_tokens = 0
    output_tokens = 0
    total_tokens = 0
    
    # AIMessageì˜ response_metadataì—ì„œ ì¶”ì¶œ
    if hasattr(result, 'usage_metadata'):
        response_metadata = result.usage_metadata
        input_tokens = response_metadata.get('input_tokens', 0)
        output_tokens = response_metadata.get('output_tokens', 0)
        total_tokens = response_metadata.get('total_tokens', 0)
        
    
    token_usage = TokenUsage(
        step_name=step_name,
        input_tokens=input_tokens,
        output_tokens=output_tokens,
        total_tokens=total_tokens
    )
    
    print(f"ğŸ“Š í† í° ì‚¬ìš©ëŸ‰ - {step_name}: ì…ë ¥={input_tokens:,}, ì¶œë ¥={output_tokens:,}, ì´={total_tokens:,}")
    
    return token_usage

# ============================================
# ì˜¤ì¼€ìŠ¤íŠ¸ë¼ ì—ì´ì „íŠ¸ (Orchestrator Agent)
# ============================================

def orchestrator_agent(state: AgenticCoderState) -> Dict[str, Any]:
    """
    ì—ì´ì „í‹± ì˜¤ì¼€ìŠ¤íŠ¸ë¼ ì—ì´ì „íŠ¸: LLMì„ ì‚¬ìš©í•˜ì—¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ì¡°ìœ¨
    
    ì—­í• :
    - í˜„ì¬ ìƒíƒœë¥¼ ë¶„ì„í•˜ê³  í‰ê°€
    - ê° ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¥¼ ê²€í† 
    - ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •
    - ì¬ì‹œë„ í•„ìš” ì—¬ë¶€ íŒë‹¨
    - ì›Œí¬í”Œë¡œìš° ì™„ë£Œ ì¡°ê±´ í‰ê°€
    
    íŠ¹ì§•:
    - ë‹¨ìˆœ ì¡°ê±´ë¬¸ì´ ì•„ë‹Œ LLM ê¸°ë°˜ ì˜ì‚¬ê²°ì •
    - ìƒí™©ì— ë§ëŠ” ì ì‘ì  íŒë‹¨
    - ëª…í™•í•œ ì´ìœ ì™€ í•¨ê»˜ ê²°ì • ì œì‹œ
    """
    print("\n" + "="*80)
    print("ğŸ¯ [Orchestrator Agent] ì›Œí¬í”Œë¡œìš° ë¶„ì„ ë° ë‹¤ìŒ ë‹¨ê³„ ê²°ì •")
    print("="*80)
    
    llm = ChatOpenAI(model="gpt-5-mini")
    
    # í˜„ì¬ ìƒíƒœ ë¶„ì„ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    current_status = state.get("current_status", "spec")
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 2)
        

    # íŒŒì¼ ìƒì„± ìƒíƒœ
    files_plan = state.get("files_plan", [])
    generated_files = state.get("generated_files", [])
    current_file_index = state.get("current_file_index", 0)
    
    if files_plan:
        code_status = f"ì§„í–‰ ì¤‘ ({len(generated_files)}/{len(files_plan)} íŒŒì¼)"
        if len(generated_files) == len(files_plan):
            code_status = "âœ… ì™„ë£Œ"
    else:
        code_status = "âŒ ê³„íš ì „"
    
    review_status = "âœ… í†µê³¼" if state.get("review_passed") else ("âŒ ì‹¤íŒ¨" if state.get("review_result") else "â³ ëŒ€ê¸°ì¤‘")
    
    print(f"ğŸ“Š í˜„ì¬ ì§„í–‰ ìƒíƒœ:")
    print(f"  - íŒŒì¼ ê³„íš: {'âœ… ì™„ë£Œ' if files_plan else 'âŒ ë¯¸ì™„ë£Œ'} ({len(files_plan) if files_plan else 0}ê°œ íŒŒì¼)")
    print(f"  - ì½”ë“œ ìƒì„±: {code_status}")
    if generated_files:
        print(f"    ìµœê·¼ ìƒì„± íŒŒì¼:")
        for gf in generated_files[-2:]:
            print(f"      - {gf['file_name']}")
    print(f"  - ì •ì  ë¦¬ë·°: {review_status}")
    print(f"  - í˜„ì¬ ìƒíƒœ: {current_status}\n")
    
    # ìƒí™© ì •ë³´ ìˆ˜ì§‘
    context_info = f"""
í˜„ì¬ ì›Œí¬í”Œë¡œìš° ìƒíƒœ:
- ë‹¨ê³„: {current_status}
- ê²€ì¦ íšŸìˆ˜: {retry_count}/{max_retries}

ì´ì „ ë‹¨ê³„ ê²°ê³¼:
{state.get("pre_result", "ì—†ìŒ")}

ì´ì „ ë‹¨ê³„ ì œì•ˆì‚¬í•­:
{state.get("previous_suggestions", "ì—†ìŒ")}

ê° ë‹¨ê³„ ì™„ë£Œ ìƒíƒœ:
- íŒŒì¼ ê³„íš: {'ì™„ë£Œ' if files_plan else 'ë¯¸ì™„ë£Œ'} (ì´ {len(files_plan) if files_plan else 0}ê°œ íŒŒì¼)
- ì½”ë“œ ìƒì„±: {code_status}
  - ìƒì„± ì™„ë£Œ: {len(generated_files) if generated_files else 0}ê°œ
  - ë‚¨ì€ íŒŒì¼: {len(files_plan) - len(generated_files) if files_plan and generated_files else 0}ê°œ
- ì •ì  ë¦¬ë·°: {review_status}
"""
    # íŒŒì¼ ê³„íš ìƒì„¸ ì •ë³´ ì¶”ê°€
    if files_plan:
        context_info += f"\níŒŒì¼ ê³„íš ìƒì„¸ ({len(files_plan)}ê°œ):\n"
        context_info += f"{files_plan}"
    
    # ìµœê·¼ ìƒì„±ëœ íŒŒì¼ ì •ë³´
    if generated_files:
        context_info += f"\nëª¨ë“  ìƒì„±ëœ íŒŒì¼:\n"
        for gf in generated_files:
            context_info += f"- {gf['file_name']}\n"
    
    # ë¦¬ë·° ê²°ê³¼ê°€ ìˆë‹¤ë©´ ì¶”ê°€
    if state.get("review_result"):
        import json
        review_data = json.loads(state["review_result"])
        issues_summary = f"ë°œê²¬ëœ ì´ìŠˆ: {len(review_data['issues'])}ê°œ"
        if review_data['issues']:
            critical_count = sum(1 for issue in review_data['issues'] if issue['severity'] == 'CRITICAL')
            major_count = sum(1 for issue in review_data['issues'] if issue['severity'] == 'MAJOR')
            issues_summary += f" (CRITICAL: {critical_count}, MAJOR: {major_count})"
        
        context_info += f"\nì •ì  ë¦¬ë·° ê²°ê³¼:\n- {issues_summary}\n- ìš”ì•½: {review_data['summary']}\n"
    
    system_prompt = """
ë‹¹ì‹ ì€ **ì½”ë“œ ìƒì„± ì›Œí¬í”Œë¡œìš°ì˜ ì´ê´„ ë§¤ë‹ˆì €ì´ì íŒŒì¼ ê³„íšì**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” í˜„ì¬ ìƒí™©ì„ ë¶„ì„í•˜ê³ , **ë‹¤ìŒ í–‰ë™ê³¼ ìƒì„±í•  íŒŒì¼ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ê²°ì •**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ë‹¹ì‹ ì€ ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì™„ì„±í•˜ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.

## ì›Œí¬í”Œë¡œìš° ë‹¨ê³„
1. **specification_writer**: ëª…ì„¸ì„œ ì‘ì„± (files_plan ìˆ˜ë¦½)
2. **code_generator**: ì½”ë“œ ìƒì„± (íŒŒì¼ í•˜ë‚˜ì”©)
3. **static_reviewer**: ì •ì  ë¦¬ë·°
4. **completed**: ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ

## ì˜ì‚¬ê²°ì • ì›ì¹™

### 1.íŒŒì¼ ê³„íš ìˆ˜ë¦½
files_planì´ ì—†ë‹¤ë©´:
- next_action: "specification_writer"

### 2. íŒŒì¼ ìƒì„± ì§„í–‰ ì¤‘
files_planì´ ìˆê³  ì•„ì§ ëª¨ë“  íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ë‹¤ë©´:
- **ë‹¤ìŒ íŒŒì¼ ê²°ì • (next_file)**
  - files_planì— ìˆëŠ” íŒŒì¼ ì¤‘ ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ íŒŒì¼ì„ next_fileë¡œ ì„¤ì •
  - ì˜ì¡´í•˜ëŠ” íŒŒì¼ì´ ëª¨ë‘ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
- **ì˜ì¡´ì„± í™•ì¸ (dependent_files)**
  - ì˜ì¡´ì„±ì´ ìˆë‹¤ë©´ ì˜ì¡´í•˜ëŠ” íŒŒì¼ ê³„íšì„ dependent_filesì— ì¶”ê°€.
- next_action: "code_generator"

### 3. ëª¨ë“  íŒŒì¼ ìƒì„± ì™„ë£Œ
ëª¨ë“  íŒŒì¼ì´ ìƒì„±ë˜ì—ˆë‹¤ë©´:
- next_action: "static_reviewer"

### 4. ë¦¬ë·° ê²°ê³¼ ë¶„ì„ ë° ìµœì¢… ê²°ì •
ë¦¬ë·°ê°€ ì™„ë£Œë˜ì—ˆë‹¤ë©´:
- **í†µê³¼ (passed=True)**: 
  - next_action: "completed"
  - final_message ìƒì„±
  
- **ì‹¤íŒ¨ (passed=False)**:
  - ì´ìŠˆ ì‹¬ê°ë„ ë¶„ì„:
    - CRITICAL ì´ìŠˆ ìˆìŒ â†’ ì¬ì‹œë„ í•„ìš”
    - MAJOR ì´ìŠˆ ë§ìŒ (3ê°œ ì´ìƒ) â†’ ì¬ì‹œë„ ê³ ë ¤
    - MINORë§Œ ìˆìŒ â†’ completed ê°€ëŠ¥

- ê²€ì¦ íšŸìˆ˜ ë„ë‹¬ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬:
  - next_action: "failed"
  - final_message ìƒì„±

## ì¶œë ¥ í˜•ì‹
- OrchestratorDecision ëª¨ë¸ë¡œ ì¶œë ¥
- **next_file**: code_generatorë¡œ ê°ˆ ë•Œë§ˆë‹¤ ë‹¤ìŒ íŒŒì¼ (FilePlan í•˜ë‚˜)
- **dependent_files**: next_fileì´ ì˜ì¡´í•˜ëŠ” íŒŒì¼ë“¤ (ì„ íƒì‚¬í•­)
- next_action, reasoning, suggestions

## ì¤‘ìš” ì›ì¹™
1. **ê³„íšì€ í•œë²ˆ**: specification_writer ëŠ” í•œë²ˆë§Œ ìˆ˜í–‰
2. **í•œë²ˆì— í•˜ë‚˜**: next_fileì€ ë§¤ë²ˆ í•œ íŒŒì¼ì”©ë§Œ ì§€ì •
3. **ì˜ì¡´ì„± ê³ ë ¤**: ì˜ì¡´í•˜ëŠ” íŒŒì¼ì´ ë¨¼ì € ìƒì„±ë˜ë„ë¡
4. **ëª…í™•í•œ ìˆœì„œ**: Entity â†’ Repository â†’ DTO â†’ Service â†’ Controller
"""
    
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", """
í˜„ì¬ ìƒí™©:
{context}

ìœ„ ìƒí™©ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•´ì£¼ì„¸ìš”.
""")
    ])
    
    chain = prompt | llm.with_structured_output(OrchestratorDecision, include_raw=True)
    
    response = chain.invoke({"context": context_info})
    
    decision = response["parsed"]
    raw_message = response["raw"]
    
    print(f"âœ… ì˜¤ì¼€ìŠ¤íŠ¸ë¼ ê²°ì • ì™„ë£Œ")
    print(f"  - ë‹¤ìŒ í–‰ë™: {decision.next_action}")
    print(f"\nğŸ“ ê²°ì • ì´ìœ :")
    print(f"  {decision.reasoning}\n")
    
    # ë‹¤ìŒ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¶œë ¥
    if decision.next_file:
        print(f"ğŸ¯ ë‹¤ìŒ ìƒì„± íŒŒì¼: {decision.next_file.file_name}")
        print(f"   ê²½ë¡œ: {decision.next_file.file_path}")
        print(f"   ì„¤ëª…: {decision.next_file.description}\n")
    
    if decision.suggestions:
        print(f"ğŸ’¡ ì œì•ˆì‚¬í•­:")
        print(f"  {decision.suggestions}")
        print()
    
    token_usage_list = state.get("token_usage_list", [])
    token_usage = extract_token_usage(raw_message, "Orchestrator Agent")
    token_usage_list.append(token_usage)

    # State ì—…ë°ì´íŠ¸
    result = {
        "current_status": decision.next_action,
        "orchestrator_reasoning": decision.reasoning,
        "token_usage_list": token_usage_list,
        "previous_suggestions": decision.suggestions
    }
    
    # next_fileì´ ìˆìœ¼ë©´ ì €ì¥
    if decision.next_file:
        result["next_file_to_generate"] = decision.next_file.model_dump()
    
    # final_messageê°€ ìˆìœ¼ë©´ ì €ì¥ (completedì¼ ë•Œ)
    if decision.final_message:
        result["final_message"] = decision.final_message
        print(f"\nğŸ“¢ ìµœì¢… ë©”ì‹œì§€: {decision.final_message}")
    
    return result


# ============================================
# 1. Specification Writer Agent
# ============================================

def specification_writer_agent(state: AgenticCoderState) -> Dict[str, Any]:
    """
    ì—­í• : ëª…ì„¸ì„œ ì‘ì„±ì
    ì…ë ¥: ì‚¬ìš©ì ìš”ì²­
    ì¶œë ¥: íŒŒì¼ê³¼ ì‹œê·¸ë‹ˆì²˜ ëª©ë¡
    
    ì£¼ìš” ì‘ì—…:
    - ì‚¬ìš©ì ìš”ì²­ ë¶„ì„
    - ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­ ë„ì¶œ
    - API ì‹œê·¸ë‹ˆì²˜ ì¶”ì¶œ
    - ê¸°ìˆ  ìŠ¤íƒ ê²°ì •
    - ì•„í‚¤í…ì²˜ ì„¤ê³„
    """
    print("\n" + "="*80)
    print("ğŸ“ [Specification Writer Agent] ëª…ì„¸ì„œ ì‘ì„± ì‹œì‘")
    print("="*80)
    
    user_request = state["user_request"]
    print(f"ì‚¬ìš©ì ìš”ì²­: {user_request}\n")
    
    llm = get_llm("gemini-2.5-pro", is_openai=False)
    
    system_prompt = """
ë‹¹ì‹ ì€ **API ì‹œê·¸ë‹ˆì²˜ ì‘ì„± ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ìš”ì²­ì„ ë°›ì•„ **API ì‹œê·¸ë‹ˆì²˜**ë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì„ë¬´ì…ë‹ˆë‹¤.
í”„ë¡œì íŠ¸ ìƒì„±ì— í•„ìš”í•œ ëª¨ë“  íŒŒì¼ê³¼ ì‹œê·¸ë‹ˆì²˜ë¥¼ ì‘ì„±í•˜ë¼. 

## í•µì‹¬ ì‘ì—…

### 1. ìš”êµ¬ì‚¬í•­ ë¶„ì„
- ëª…ì‹œëœ ê¸°ëŠ¥ê³¼ ì•”ì‹œëœ ê¸°ëŠ¥ì„ ëª¨ë‘ íŒŒì•…
- í•„ìš”í•œ ë„ë©”ì¸ ëª¨ë¸ ì‹ë³„
- ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì •ì˜

### 2. API ì‹œê·¸ë‹ˆì²˜ ì‘ì„±
- íŒŒì¼ëª…ê³¼ API ì‹œê·¸ë‹ˆì²˜ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì‘ì„±
- ì˜ì¡´í•˜ëŠ” íŒŒì¼ëª…ë„ ëª…ì‹œ
api_signatures í•„ë“œì˜ signature í•„ë“œëŠ” ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ ì‘ì„±í•˜ë¼.
ì™„ì„±ëœ ì½”ë“œê°€ ì•„ë‹Œ ì¸í„°í˜ì´ìŠ¤ì˜ í˜•íƒœì²˜ëŸ¼ ì‘ì„±í•˜ë¼.
Class Todo 
    id: Long,
    title: String,
    description: String,
    priority: Int

Class TodoService
    getTodo(Long id): Todo, 
    createTodo(Todo todo): Todo

### 3. ê¸°ìˆ  ìŠ¤íƒ ê²°ì •
- Spring Boot ê¸°ë°˜ (Java 17, Spring Boot 3.x)
- í•„ìš”í•œ ì˜ì¡´ì„± ëª…ì‹œ (JPA, Security, Validation ë“±)
- ë°ì´í„°ë² ì´ìŠ¤(H2 ê³ ì •) ë° ê¸°íƒ€ ì¸í”„ë¼

## ì¶œë ¥ í˜•ì‹
- ì£¼ì–´ì§„ Pydantic ëª¨ë¸(Specification) í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
- API ì‹œê·¸ë‹ˆì²˜ëŠ” APISignature ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬ì¡°í™”
- ëª…í™•í•˜ê³  êµ¬í˜„ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì„±

## ì¤‘ìš” ì›ì¹™
1. **êµ¬ì²´ì„±**: ëª¨í˜¸í•œ í‘œí˜„ ê¸ˆì§€, êµ¬í˜„ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì‘ì„±
2. **ì™„ì „ì„±**: ëª¨ë“  í•„ìš”í•œ APIì™€ ê¸°ëŠ¥ì„ ë¹ ì§ì—†ì´ í¬í•¨
3. **ì¼ê´€ì„±**: ëª…ëª… ê·œì¹™, ì‘ë‹µ í˜•ì‹ ë“± ì¼ê´€ì„± ìœ ì§€
4. **ì‹¤ìš©ì„±**: ê³¼ë„í•œ ì„¤ê³„ ì§€ì–‘, MVP ìˆ˜ì¤€ì˜ ì‹¤ìš©ì  ì„¤ê³„
"""
    
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì ìš”ì²­: {user_request}\n\nìœ„ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì™„ì „í•œ API ì‹œê·¸ë‹ˆì²˜ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.")
    ])
    
    chain = prompt | llm.with_structured_output(Specification, include_raw=True)
    
    response = chain.invoke({"user_request": user_request})
    
    specification = response["parsed"]
    raw_message = response["raw"]
    
    print(f"âœ… API ì‹œê·¸ë‹ˆì²˜ ì‘ì„± ì™„ë£Œ")
    print(f"  - í”„ë¡œì íŠ¸: {specification.title}")
    print(f"  - API ê°œìˆ˜: {len(specification.api_signatures)}ê°œ")
    print(f"  - ê¸°ìˆ  ìŠ¤íƒ: {specification.technical_stack}\n")

    print(f"ğŸ“‹ íŒŒì¼ ê³„íš ëª©ë¡:")
    for i, file_plan in enumerate(specification.api_signatures):
        print(f"  - {i+1}. {file_plan.file_name}: {file_plan.description}")

    token_usage_list = state.get("token_usage_list", [])
    token_usage = extract_token_usage(raw_message, "Specification Writer Agent")
    token_usage_list.append(token_usage)

    sum_result = [f"{content.file_name}: {content.description}" for content in specification.api_signatures]
    sum_result = "\n".join(sum_result)

    return {
        "files_plan": specification.api_signatures,
        "pre_result": sum_result,
        "current_status": "orchestrator",  # ì˜¤ì¼€ìŠ¤íŠ¸ë¼ê°€ íŒë‹¨í•˜ë„ë¡
        "token_usage_list": token_usage_list
    }

# ============================================
# 2. Code Generator Agent (ë‹¨ì¼ íŒŒì¼ ìƒì„±)
# ============================================

def code_generator_agent(state: AgenticCoderState) -> Dict[str, Any]:
    """
    ì—­í• : ë‹¨ì¼ íŒŒì¼ ì½”ë“œ ìƒì„±ì
    ì…ë ¥: ì˜¤ì¼€ìŠ¤íŠ¸ë¼ê°€ ì§€ì •í•œ íŒŒì¼ ì •ë³´ (next_file_to_generate)
    ì¶œë ¥: ë‹¨ì¼ íŒŒì¼ ì½”ë“œ
    
    ì£¼ìš” ì‘ì—…:
    - ì˜¤ì¼€ìŠ¤íŠ¸ë¼ê°€ ì§€ì •í•œ íŒŒì¼ í•˜ë‚˜ë§Œ ìƒì„±
    - ì´ë¯¸ ìƒì„±ëœ íŒŒì¼ë“¤(generated_files)ì„ ì°¸ì¡°
    - ëª…ì„¸ì„œì™€ ì¼ê´€ì„± ìœ ì§€
    """
    print("\n" + "="*80)
    print("ğŸ’» [Code Generator Agent] ë‹¨ì¼ íŒŒì¼ ìƒì„±")
    print("="*80)
    
    import json
    
    next_file = state.get("next_file_to_generate")
    if not next_file:
        print("âš ï¸ ìƒì„±í•  íŒŒì¼ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"current_status": "orchestrator"}
    
    generated_files = state.get("generated_files", [])
    
    print(f"ìƒì„±í•  íŒŒì¼: {next_file['file_name']}")
    print(f"ê²½ë¡œ: {next_file['file_path']}")
    print(f"ì„¤ëª…: {next_file['description']}")
    print(f"ì´ë¯¸ ìƒì„±ëœ íŒŒì¼: {len(generated_files)}ê°œ\n")
    
    llm = get_llm()
    
    system_prompt = """
ë‹¹ì‹ ì€ **Spring Boot ì „ë¬¸ ê°œë°œì**ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ëª…ì„¸ì„œì™€ íŒŒì¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **ë‹¨ì¼ íŒŒì¼ì˜ ì™„ì „í•œ ì½”ë“œ**ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì„ë¬´ì…ë‹ˆë‹¤.

## í•µì‹¬ ì‘ì—…

### 1. í˜„ì¬ íŒŒì¼ë§Œ ìƒì„±
- ì£¼ì–´ì§„ íŒŒì¼ í•˜ë‚˜ë§Œ ì§‘ì¤‘
- íŒŒì¼ íƒ€ì…ì— ë§ëŠ” êµ¬í˜„:
  - **Entity**: JPA ì—”í‹°í‹° (@Entity, @Id, Lombok)
  - **Repository**: Spring Data JPA ì¸í„°í˜ì´ìŠ¤
  - **DTO**: Request/Response ê°ì²´ (Validation í¬í•¨)
  - **Service**: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ (@Service, @Transactional)
  - **Controller**: REST API ì—”ë“œí¬ì¸íŠ¸
  - **Exception**: ì»¤ìŠ¤í…€ ì˜ˆì™¸, ê¸€ë¡œë²Œ í•¸ë“¤ëŸ¬

### 2. ì´ë¯¸ ìƒì„±ëœ íŒŒì¼ í™œìš©
- ì˜ì¡´í•˜ëŠ” íŒŒì¼ë“¤ì˜ ì½”ë“œ ì°¸ì¡°
- í´ë˜ìŠ¤ëª…, íŒ¨í‚¤ì§€ëª…, ìŠ¤íƒ€ì¼ ì¼ê´€ì„± ìœ ì§€
- íƒ€ì… í˜¸í™˜ì„± ë³´ì¥

### 3. ëª…ì„¸ì„œ ì¤€ìˆ˜
- API ì‹œê·¸ë‹ˆì²˜ ì •í™•íˆ êµ¬í˜„
- HTTP ë©”ì„œë“œ, ê²½ë¡œ, ìš”ì²­/ì‘ë‹µ í˜•ì‹ ì¼ì¹˜

### 4. ì½”ë“œ í’ˆì§ˆ
- Spring Boot ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
- ì ì ˆí•œ ì˜ˆì™¸ ì²˜ë¦¬
- Validation ì–´ë…¸í…Œì´ì…˜
- ê¹”ë”í•˜ê³  ìëª…í•œ ì½”ë“œ

## ì¶œë ¥ í˜•ì‹
- SingleFileGeneration ëª¨ë¸ë¡œ ì¶œë ¥
- file_name, file_path, code_content
- ì™„ì „í•œ Java ì½”ë“œ (importë¶€í„° ëê¹Œì§€)

## ì¤‘ìš” ì›ì¹™
1. **ì™„ì „ì„±**: ëª¨ë“  import, ì–´ë…¸í…Œì´ì…˜, ë©”ì„œë“œ í¬í•¨
2. **ì¼ê´€ì„±**: ê¸°ì¡´ íŒŒì¼ë“¤ê³¼ ìŠ¤íƒ€ì¼ ì¼ì¹˜
3. **ì‹¤í–‰ ê°€ëŠ¥ì„±**: ì»´íŒŒì¼ ê°€ëŠ¥í•œ ì½”ë“œ
4. **ì§‘ì¤‘**: í˜„ì¬ íŒŒì¼ë§Œ ìƒì„±
"""
    
    # ì˜ì¡´ íŒŒì¼ ì»¨í…ìŠ¤íŠ¸
    dependency_context = ""
    if next_file.get("dependencies") and generated_files:
        dependency_context = "\n\n### ì°¸ê³ : ì˜ì¡´í•˜ëŠ” íŒŒì¼ë“¤\n"
        for dep_name in next_file["dependencies"]:
            for gen_file in generated_files:
                if gen_file["file_name"] == dep_name:
                    dependency_context += f"\n// {gen_file['file_name']}\n"
                    dependency_context += f"{gen_file['code_content'][:800]}...\n"
                    break
    
    # ìƒì„±ëœ íŒŒì¼ ìš”ì•½
    generated_summary = ""
    if generated_files:
        generated_summary = f"\n\n### ì´ë¯¸ ìƒì„±ëœ íŒŒì¼ ({len(generated_files)}ê°œ):\n"
        for gf in generated_files:
            generated_summary += f"- {gf['file_name']} ({gf['file_path']})\n"
    
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", """
ë‹¤ìŒ íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

ìƒì„±í•  íŒŒì¼:
- íŒŒì¼ëª…: {file_name}
- ê²½ë¡œ: {file_path}
- ì„¤ëª…: {description}
ë°˜ë“œì‹œ ë‹¤ìŒ ì‹œê·¸ë‹ˆì²˜ë¥¼ ì¤€ìˆ˜í•˜ë¼.
{signature}
{generated_summary}
{dependency_context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {file_name} íŒŒì¼ì˜ ì™„ì „í•œ ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
""")
    ])
    
    chain = prompt | llm.with_structured_output(SingleFileGeneration, include_raw=True)
    
    response = chain.invoke({
        "file_name": next_file["file_name"],
        "file_path": next_file["file_path"],
        "signature": next_file["signature"],
        "description": next_file["description"],
        "generated_summary": generated_summary,
        "dependency_context": dependency_context
    })
    
    file_gen = response["parsed"]
    raw_message = response["raw"]
    
    print(f"âœ… íŒŒì¼ ìƒì„± ì™„ë£Œ")
    print(f"  - íŒŒì¼: {file_gen.file_name}")
    print(f"  - ì½”ë“œ ê¸¸ì´: {len(file_gen.code_content)} ì\n")

    print(f"ğŸ“‹ ìƒì„±ëœ ì½”ë“œ:")
    print(f"  - {file_gen.code_content}")
    print("="*80)
    
    # ìƒì„±ëœ íŒŒì¼ ì¶”ê°€
    new_file = {
        "file_name": file_gen.file_name,
        "file_path": file_gen.file_path,
        "code_content": file_gen.code_content,
        "description": next_file["description"]
    }
    
    updated_generated_files = generated_files + [new_file]
    current_index = state.get("current_file_index", 0)

    token_usage_list = state.get("token_usage_list", [])
    token_usage = extract_token_usage(raw_message, "Code Generator Agent")
    token_usage_list.append(token_usage)
    
    return {
        "pre_result": "generated: " + file_gen.file_name + ": " + next_file["description"],
        "current_file_code": file_gen.code_content,
        "generated_files": updated_generated_files,
        "current_file_index": current_index + 1,
        "current_status": "orchestrator",  # ì˜¤ì¼€ìŠ¤íŠ¸ë¼ê°€ ë‹¤ìŒ íŒë‹¨
        "token_usage_list": token_usage_list
    }


# ============================================
# 3. Static Reviewer Agent
# ============================================

def static_reviewer_agent(state: AgenticCoderState) -> Dict[str, Any]:
    """
    ì—­í• : ì •ì  ë¶„ì„ ë° ë¦¬ë·°ì–´
    ì…ë ¥: ìƒì„±ëœ ì½”ë“œ
    ì¶œë ¥: ë¦¬ë·° ê²°ê³¼ (ì´ìŠˆ ëª©ë¡, í†µê³¼ ì—¬ë¶€)
    
    ì£¼ìš” ì‘ì—…:
    - ì½”ë“œ ì •ì  ë¶„ì„
    - ì ì¬ì  ë²„ê·¸ íƒì§€ (Null Pointer, Resource Leak ë“±)
    - ë³´ì•ˆ ì·¨ì•½ì  ê²€ì‚¬
    - ì½”ë“œ ìŠ¤ë©œ íƒì§€
    - ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì¤€ìˆ˜ ì—¬ë¶€
    - ê°œì„  ì œì•ˆ
    """
    print("\n" + "="*80)
    print("ğŸ” [Static Reviewer Agent] ì •ì  ë¦¬ë·° ì‹œì‘")
    print("="*80)
    
    generated_files = state.get("generated_files", [])
    
    if not generated_files:
        print("âš ï¸ ë¦¬ë·°í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {"current_status": "completed"}
    
    print(f"ë¦¬ë·°í•  íŒŒì¼ ìˆ˜: {len(generated_files)}ê°œ\n")
    
    # ëª¨ë“  ì½”ë“œë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
    all_code = ""
    for file in generated_files:
        all_code += f"\n\n{'='*80}\n"
        all_code += f"íŒŒì¼: {file['file_path']}/{file['file_name']}\n"
        all_code += f"{'='*80}\n"
        all_code += file['code_content']
    
    llm = get_llm()
    
    system_prompt = """
ë‹¹ì‹ ì€ **ì‹œë‹ˆì–´ Java/Spring Boot ê°œë°œìì´ì ì½”ë“œ ë¦¬ë·° ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.

ìƒì„±ëœ ì½”ë“œë¥¼ **ì •ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ë¦¬ë·°**í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì„ë¬´ì…ë‹ˆë‹¤.

## ê²€ì‚¬ í•­ëª©

### 1. ì ì¬ì  ë²„ê·¸ (CRITICAL/MAJOR)
- **NullPointerException ìœ„í—˜**
  - Optional ì²˜ë¦¬ ëˆ„ë½
  - Null ì²´í¬ ì—†ëŠ” ë©”ì„œë“œ í˜¸ì¶œ
  - findById(), orElseThrow() ë“± ì ì ˆí•œ ì²˜ë¦¬ ì—¬ë¶€
  
- **Resource Leak**
  - Stream, Connection ë“± ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  
- **ë™ì‹œì„± ì´ìŠˆ**
  - Thread-safety ë¬¸ì œ
  - ê³µìœ  ìƒíƒœ ê´€ë¦¬

### 2. ë³´ì•ˆ ì·¨ì•½ì  (CRITICAL/MAJOR)
- **SQL Injection**: JPQL, Native Query ê²€ì‚¬
- **ì¸ì¦/ì¸ê°€ ëˆ„ë½**: ë³´ì•ˆì´ í•„ìš”í•œ APIì— @PreAuthorize ë“± ëˆ„ë½
- **ë¯¼ê° ì •ë³´ ë…¸ì¶œ**: ë¹„ë°€ë²ˆí˜¸ í‰ë¬¸ ì €ì¥, ë¡œê·¸ì— ë¯¼ê°ì •ë³´ ì¶œë ¥
- **CSRF, XSS ëŒ€ì‘**

### 3. Spring Boot ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ (MAJOR/MINOR)
- **ì˜ì¡´ì„± ì£¼ì…**: ìƒì„±ì ì£¼ì… ì‚¬ìš© (í•„ë“œ ì£¼ì… ì§€ì–‘)
- **íŠ¸ëœì­ì…˜**: @Transactional ì ì ˆí•œ ìœ„ì¹˜ ë° ì˜µì…˜
- **ì˜ˆì™¸ ì²˜ë¦¬**: Custom Exception, @ControllerAdvice í™œìš©
- **Validation**: @Valid, @NotNull ë“± ì ì ˆí•œ ì‚¬ìš©
- **Layered Architecture**: ë ˆì´ì–´ ê°„ ì±…ì„ ë¶„ë¦¬

### 4. ì½”ë“œ ìŠ¤ë©œ (MINOR/INFO)
- **ê¸´ ë©”ì„œë“œ**: ë©”ì„œë“œê°€ ë„ˆë¬´ ê¸´ ê²½ìš°
- **ì¤‘ë³µ ì½”ë“œ**: ë°˜ë³µë˜ëŠ” ë¡œì§
- **ë§¤ì§ ë„˜ë²„/ë¬¸ìì—´**: ìƒìˆ˜í™” í•„ìš”
- **ê³¼ë„í•œ ê²°í•©ë„**: í´ë˜ìŠ¤ ê°„ ì˜ì¡´ì„± ê³¼ë‹¤

### 5. ëª…ëª… ë° ì»¨ë²¤ì…˜ (MINOR)
- **ëª…ëª… ê·œì¹™**: í´ë˜ìŠ¤, ë©”ì„œë“œ, ë³€ìˆ˜ëª… ì ì ˆì„±
- **Java ì»¨ë²¤ì…˜**: Camel Case, Pascal Case ë“±

## ì‹¬ê°ë„ ë¶„ë¥˜
- **CRITICAL**: ì¦‰ì‹œ ìˆ˜ì • í•„ìš” (ë³´ì•ˆ, ì¹˜ëª…ì  ë²„ê·¸)
- **MAJOR**: ë°˜ë“œì‹œ ìˆ˜ì • ê¶Œì¥ (ì ì¬ì  ë²„ê·¸, ì¤‘ìš” ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤)
- **MINOR**: ê°œì„  ê¶Œì¥ (ì½”ë“œ í’ˆì§ˆ, ê°€ë…ì„±)
- **INFO**: ì°¸ê³ ì‚¬í•­ (ìµœì í™” ì œì•ˆ ë“±)

## ë¦¬ë·° í†µê³¼ ê¸°ì¤€
- CRITICAL ì´ìŠˆ: 0ê°œ
- MAJOR ì´ìŠˆ: 3ê°œ ì´í•˜
- ìœ„ ê¸°ì¤€ì„ ë§Œì¡±í•˜ë©´ passed=True, ì•„ë‹ˆë©´ passed=False

## ì¶œë ¥ í˜•ì‹
- ì£¼ì–´ì§„ Pydantic ëª¨ë¸(StaticReviewResult) í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
- ê° ì´ìŠˆëŠ” CodeIssue í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”
- êµ¬ì²´ì ì¸ íŒŒì¼ëª…, ì¤„ ë²ˆí˜¸(ê°€ëŠ¥í•œ ê²½ìš°), ì´ìŠˆ ì„¤ëª…, ê°œì„  ì œì•ˆ í¬í•¨

## ì¤‘ìš” ì›ì¹™
1. **êµ¬ì²´ì„±**: "ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤" X, "UserService.javaì˜ findById() í˜¸ì¶œ ì‹œ null ì²´í¬ ëˆ„ë½" O
2. **ì‹¤ìš©ì„±**: ì‚¬ì†Œí•œ ì´ìŠˆë³´ë‹¤ ì¤‘ìš”í•œ ì´ìŠˆì— ì§‘ì¤‘
3. **ê±´ì„¤ì **: ë¹„íŒë³´ë‹¤ëŠ” ê°œì„  ì œì•ˆ ì¤‘ì‹¬
4. **ì •í™•ì„±**: ì‹¤ì œ ë¬¸ì œë§Œ ì§€ì , ì˜¤íƒ ìµœì†Œí™”
"""
    
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", """
ë‹¤ìŒ ìƒì„±ëœ ì½”ë“œë¥¼ ì •ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ë¦¬ë·°í•´ì£¼ì„¸ìš”.

ìƒì„±ëœ ì½”ë“œ:
{generated_code}

ëª¨ë“  ì ì¬ì  ì´ìŠˆë¥¼ ì°¾ì•„ë‚´ê³ , ì‹¬ê°ë„ë¥¼ í‰ê°€í•˜ë©°, êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆì„ ì œê³µí•´ì£¼ì„¸ìš”.
""")
    ])
    
    chain = prompt | llm.with_structured_output(StaticReviewResult, include_raw=True)
    
    response = chain.invoke({"generated_code": all_code})
    
    review_result = response["parsed"]
    raw_message = response["raw"]
    
    print(f"âœ… ì •ì  ë¦¬ë·° ì™„ë£Œ")
    print(f"  - í†µê³¼ ì—¬ë¶€: {'âœ… PASS' if review_result.passed else 'âŒ FAIL'}")
    print(f"  - ë°œê²¬ëœ ì´ìŠˆ: {len(review_result.issues)}ê°œ")
    print(f"  - ìš”ì•½: {review_result.summary}\n")
    
    if review_result.issues:
        print("ğŸ” ë°œê²¬ëœ ì´ìŠˆ:")
        for issue in review_result.issues:
            severity_emoji = {
                "CRITICAL": "ğŸ”´",
                "MAJOR": "ğŸŸ ",
                "MINOR": "ğŸŸ¡",
                "INFO": "ğŸ”µ"
            }.get(issue.severity, "âšª")
            
            location = f"{issue.file_name}"
            if issue.line_number:
                location += f":{issue.line_number}"
            
            print(f"  {severity_emoji} [{issue.severity}] {location}")
            print(f"     {issue.issue_type}: {issue.description}")
            if issue.suggestion:
                print(f"     ğŸ’¡ ì œì•ˆ: {issue.suggestion}")
            print()
    
    if review_result.recommendations:
        print("ğŸ“Œ ì „ë°˜ì ì¸ ê°œì„  ê¶Œì¥ì‚¬í•­:")
        for rec in review_result.recommendations:
            print(f"  - {rec}")
    
    # ë¦¬ë·° ê²°ê³¼ ë°˜í™˜ (ë‹¤ìŒ í–‰ë™ì€ ì˜¤ì¼€ìŠ¤íŠ¸ë¼ê°€ ê²°ì •)
    print(f"\nğŸ“‹ ë¦¬ë·° ì™„ë£Œ. ì˜¤ì¼€ìŠ¤íŠ¸ë¼ì—ê²Œ ê²°ê³¼ ì „ë‹¬...")
    
    token_usage_list = state.get("token_usage_list", [])
    token_usage = extract_token_usage(raw_message, "Static Reviewer Agent")
    token_usage_list.append(token_usage)
    
    return {
        "review_result": review_result.model_dump_json(indent=2),
        "review_passed": review_result.passed,
        "issues_found": [f"[{issue.severity}] {issue.file_name}: {issue.description}" 
                        for issue in review_result.issues],
        "current_status": "orchestrator",  # ì˜¤ì¼€ìŠ¤íŠ¸ë¼ê°€ ë‹¤ìŒ ê²°ì •
        "code_files": generated_files,  # ìµœì¢… ê²°ê³¼ ì €ì¥
        "retry_count": state.get("retry_count", 0) + 1,
        "token_usage_list": token_usage_list
    }

