import os
import json
from dotenv import load_dotenv
import operator
from typing import TypedDict, Annotated, List, Optional

# Google Cloud ê´€ë ¨ ê²½ê³  ë©”ì‹œì§€ ì–µì œ
os.environ['GRPC_VERBOSITY'] = 'ERROR'

from pydantic import BaseModel, Field
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, START, END

class AnalysisResult(BaseModel):
    goal: str = Field(description="ë¶„ì„ëœ ê³ ê°ì˜ ìš”ì²­ì‚¬í•­ì— ëŒ€í•œ ëª©í‘œ")
    functional_requirements: str = Field(description="ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­")
    non_functional_requirements: str = Field(description="ë¹„ ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­")
    is_complete: bool = Field(description="ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ì™„ë£Œí•˜ê¸°ì— ì •ë³´ê°€ ì¶©ë¶„í•˜ë©´ true(í™•ì •), ë¶€ì¡±í•˜ë©´ false(ì¶”ê°€ ì •ë³´ ìš”ì²­)")
    question_to_user: Optional[str] = Field(description="ì •ë³´ê°€ ë¶€ì¡±í•  ê²½ìš°, ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸")

class UserQuestion(BaseModel):
    question_to_user: str = Field(description="ì‚¬ìš©ìì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸")
    is_complete: bool = Field(description="ìš”êµ¬ì‚¬í•­ ë¶„ì„ì„ ì™„ë£Œí•˜ê¸°ì— ì •ë³´ê°€ ì¶©ë¶„í•˜ë©´ true, ë¶€ì¡±í•˜ë©´ false")

class AgentState(TypedDict):
    request: str
    functional_requirements: str
    non_functional_requirements: str
    is_complete: bool
    question_to_user: Optional[str]
    messages: Annotated[list, operator.add]

def request_analysis(state: AgentState):
    """ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ë¶„ì„í•˜ê³ , ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” ë…¸ë“œ"""
    print("--- ğŸ“ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘... ---")


    parser = JsonOutputParser(pydantic_object=AnalysisResult)
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³  ìš”êµ¬ì‚¬í•­ì„ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ ì •ë¦¬í•˜ë¼.
    í•œë²ˆ í™•ì •ëœ ìš”êµ¬ì‚¬í•­ì€ ë‹¤ì‹œ ìˆ˜ì •í•˜ì§€ ì•Šê³  êµ¬í˜„í•˜ê¸° ë•Œë¬¸ì— ì¶”í›„ì— ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì§€ ì•Šë„ë¡ ìµœëŒ€í•œ ì •í™•í•˜ê³  ì„¸ì„¸í•˜ê²Œ ì‘ì„±í•´ì•¼í•œë‹¤.
    ë¨¼ì € ì„ì˜ë¡œ í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì— ëŒ€í•´ ìš”êµ¬ì‚¬í•­ì„ ì‘ì„±í•˜ê³ , ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë°›ì•„ ìš”êµ¬ì‚¬í•­ì„ ìˆ˜ì •í•œë‹¤.
    
    [ëª©í‘œ]
    - ê³ ê°ì˜ ì¶”ìƒì  ìš”êµ¬ë¥¼ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ì •ì œí•œë‹¤.
    - ê¸°ëŠ¥ì /ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ë¥¼ ëª…í™•íˆ ë¶„ë¦¬í•˜ê³ , ëª¨í˜¸ì„±ì„ ì œê±°í•œë‹¤.
    - ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ í•µì‹¬ ê³µë°±ì„ ì‹ë³„í•˜ê³  ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ë³´ì™„í•œë‹¤.

    [ì‘ì„± ê·œì¹™]
    - ê¸°ëŠ¥/ë¹„ê¸°ëŠ¥ ìš”êµ¬ëŠ” ë²ˆí˜¸ ëª©ë¡ í˜•íƒœë¡œ ì‘ì„±í•˜ë˜, ê° í•­ëª©ì€ ì¸¡ì • ê°€ëŠ¥í•˜ê±°ë‚˜ ê²€ì¦ ê°€ëŠ¥í•œ ìˆ˜ì¹˜/ì¡°ê±´ì„ í¬í•¨í•œë‹¤. ì˜ˆ: "ì‘ë‹µ ì‹œê°„ p95 â‰¤ 300ms", "ë™ì‹œ ì ‘ì† 5,000 ì‚¬ìš©ì ì§€ì›".
    - ëª¨í˜¸í•œ í‘œí˜„(ë¹ ë¥´ê²Œ, í¬ê²Œ, ì•ˆì •ì  ë“±)ì€ ê¸ˆì§€í•˜ê³ , êµ¬ì²´ì  ìˆ˜ì¹˜/ì¡°ê±´/ì‚¬ë¡€ë¡œ ëŒ€ì²´í•œë‹¤.
    - ë²”ìœ„(Scope), ê°€ì •(Assumptions), ì œì•½(Constraints)ì´ ì•”ì‹œë˜ì–´ ìˆìœ¼ë©´ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ê³  í•´ë‹¹ í•­ëª©ì— í†µí•©í•œë‹¤.
    - ë¦¬ìŠ¤í¬ì™€ ë¶ˆí™•ì‹¤ì„±ì´ ë³´ì´ë©´ í•´ë‹¹ í•­ëª©ì„ ì§ˆë¬¸ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ê³ ê° í™•ì¸ì´ í•„ìš”í•¨ì„ í‘œì‹œí•œë‹¤.
    - ë³´ì•ˆ/ê°œì¸ì •ë³´, ë¡œê¹…/ëª¨ë‹ˆí„°ë§, ë°°í¬/ë¡¤ë°±, êµ­ì œí™”/í˜„ì§€í™”, ì ‘ê·¼ì„± ë“± ì¼ë°˜ì ìœ¼ë¡œ ëˆ„ë½ë˜ê¸° ì‰¬ìš´ ë¹„ê¸°ëŠ¥ í•­ëª©ì„ ìŠµê´€ì ìœ¼ë¡œ ì ê²€í•œë‹¤.
    - ëŒ€í™” ì´ë ¥ì˜ í•µì‹¬ ìš”êµ¬ì™€ ë¹„í•µì‹¬ ì¡ìŒì„ êµ¬ë¶„í•˜ì—¬, í•µì‹¬ë§Œ ë°˜ì˜í•œë‹¤.

    [ëŒ€í™” í™œìš© ë°©ë²•]
    - ë¨¼ì € ëŒ€í™”(conversation_history)ì˜ ëª©ì /ë°°ê²½/ì œì•½ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ í•¨ì¶• ìš”ì•½í•œ ë’¤, ìš”êµ¬ì‚¬í•­ì„ ì •ë¦¬í•œë‹¤.
    - ë¶ˆì¶©ë¶„í•œ ë¶€ë¶„ì´ ìˆìœ¼ë©´ question_to_userì— ìµœì†Œ 3ê°œ ì´ìƒ êµ¬ì²´ì ì´ê³  ë‹µí•˜ê¸° ì‰¬ìš´ ì§ˆë¬¸ì„ ì œì‹œí•œë‹¤. ë‹¨, ì •ë³´ê°€ ì¶©ë¶„í•˜ë©´ ì§ˆë¬¸ ê°œìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆ ì§€ì¹¨ì„ ì—„ê²©íˆ ë”°ë¥¸ë‹¤. í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.


    í˜„ì¬ ê³„íšëœ ê¸°ëŠ¥: {functional_requirements}
    í˜„ì¬ ê³„íšëœ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬: {non_functional_requirements}


    {format_instructions}
    """

    user_request = """
    ìš”ì²­ëœ ìš”êµ¬ì‚¬í•­: {question_to_user}
    ê·¸ì— ë”°ë¥¸ ì‚¬ìš©ì ì‘ë‹µ: {request}
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", user_request)
    ])

    chain = prompt | llm | parser

    result = chain.invoke({
        "format_instructions": parser.get_format_instructions(),
        "request": state["request"],    
        "question_to_user": state["question_to_user"],
        "functional_requirements": state.get("functional_requirements", "ì•„ì§ ì •ì˜ë˜ì§€ ì•ŠìŒ"),
        "non_functional_requirements": state.get("non_functional_requirements", "ì•„ì§ ì •ì˜ë˜ì§€ ì•ŠìŒ"),
    })

    return {
        "request": result["goal"], 
        "functional_requirements": result["functional_requirements"], 
        "non_functional_requirements": result["non_functional_requirements"], 
        "is_complete": result["is_complete"], 
        "question_to_user": result["question_to_user"]
    }

def ask_to_user(state: AgentState):
    """ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ë…¸ë“œ"""

    print("--- ğŸ“ ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­ ì¤‘... ---")
    parser = JsonOutputParser(pydantic_object=UserQuestion)
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
   

    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ê°œë°œíŒ€ì´ ìš”ì²­í•œ ì •ë³´ì™€ ëŒ€í™” ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ë¡œ í™•ì¸í•´ì•¼ í•  ì§ˆë¬¸ì„ ìƒì„±í•œë‹¤.
    
    
    [ì§€ì¹¨]
    - ëŒ€í™” ì´ë ¥(conversation_history)ì˜ ë§¥ë½ì„ ë°˜ì˜í•˜ë˜, ì´ë¯¸ ë¬¼ì–´ë³¸ ì§ˆë¬¸ì€ ë°˜ë³µí•˜ì§€ ì•ŠëŠ”ë‹¤.
    - ì§ˆë¬¸ì€ êµ¬ì²´ì ì´ê³  ë‹µí•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ë©°, ì„ íƒì§€ë‚˜ ì˜ˆì‹œë¥¼ ë§ë¶™ì—¬ ì‘ë‹µ í’ˆì§ˆì„ ë†’ì¸ë‹¤.
    - í•œ ë²ˆì— ë„ˆë¬´ ë§ì€ ì§ˆë¬¸ì„ ì£¼ì§€ ë§ê³  ìµœëŒ€ 3-5ê°œë¡œ ìš°ì„ ìˆœìœ„ë¥¼ ë°˜ì˜í•œë‹¤.
    - ê° ì§ˆë¬¸ì€ ë‹¨ì¼ ì´ìŠˆë§Œ ë‹¤ë£¨ë„ë¡ ë¶„ë¦¬í•œë‹¤.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆ ì§€ì¹¨ì„ ì—„ê²©íˆ ë”°ë¥¸ë‹¤. í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.
      - question_to_user: ì‚¬ìš©ìì—ê²Œ ë³´ë‚¼ ì§ˆë¬¸(ë²ˆí˜¸ ëª©ë¡). ì¤‘ë³µ/ëª¨í˜¸ ê¸ˆì§€.
      - is_complete: ì •ë³´ê°€ ì¶©ë¶„í•´ êµ¬í˜„ ê³„íš ìˆ˜ë¦½ì´ ê°€ëŠ¥í•˜ë©´ true, ì•„ë‹ˆë©´ false.

  
    {format_instructions}
    """

    chat_history = state["messages"]

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”."),
    ])

    chain = prompt | llm | parser
    result = chain.invoke({
        "format_instructions": parser.get_format_instructions(),
        "chat_history": chat_history
    })

    return {
        "messages": [AIMessage(content=result["question_to_user"])], 
        "question_to_user": result["question_to_user"], 
        "is_complete": result["is_complete"]
    }
    

def user_response(state: AgentState):
    """ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë°›ëŠ” ë…¸ë“œ"""

    print("--- ğŸ“ ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë°›ëŠ” ì¤‘... ---")
    print(f"\nâ“ {state['question_to_user']}")
    response = input("ğŸ‘¤ ë‹µë³€: ")

    return {"request": response, "messages": [HumanMessage(content=response)]}

def is_complete(state: AgentState):
    return state["is_complete"]

def main(first_request: str):
   
    load_dotenv()

    # ê·¸ë˜í”„ ìƒì„±
    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("request_analysis", request_analysis)
    workflow.add_node("ask_to_user", ask_to_user)
    workflow.add_node("user_response", user_response)

    workflow.add_edge(START, "request_analysis") 
    workflow.add_conditional_edges("request_analysis", is_complete,{True: END, False: "ask_to_user"})    
    workflow.add_edge("ask_to_user", "user_response")
    workflow.add_edge("user_response", "request_analysis")

    app = workflow.compile()

    initial_state = {
        "request": first_request,
        "functional_requirements": "",
        "non_functional_requirements": "",
        "is_complete": False,
        "question_to_user": None,
        "messages": [HumanMessage(content=first_request)]
    }
    
    final_state = app.invoke(initial_state)

    print("\n" + "="*60)
    print("âœ… ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì™„ë£Œ!")
    print("="*60)
    print(f"\nğŸ“‹ ëª©í‘œ: {final_state['request']}")
    print(f"\nğŸ“Œ ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­:\n{final_state['functional_requirements']}")
    print(f"\nâš™ï¸ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­:\n{final_state['non_functional_requirements']}")
    
    return final_state
    

if __name__ == "__main__":
    print("ğŸ’¡ ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”:")
    first_request = input("ğŸ‘‰ ")
    main(first_request)