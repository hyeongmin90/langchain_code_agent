import os
import json
from dotenv import load_dotenv
import operator
from typing import TypedDict, Annotated, List, Optional
from pydantic import BaseModel, Field
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, START, END
from schemas import DecomposeAgentState, UserStoriesResult, RefinedUserStoriesResult, FinalUserStoriesResult, RefinedUserStoriesDraft, NonFunctionalRequirements, ScopeAndConstraints
import asyncio
import pprint

def decompose_into_user_stories(state: DecomposeAgentState):
    print("--- ğŸ“ ìœ ì € ìŠ¤í† ë¦¬ ì´ˆì•ˆ ìƒì„± ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ì˜ ë¸Œë ˆì¸ìŠ¤í† ë° ì„¸ì…˜ì„ ì´ë„ëŠ”, ì°½ì˜ì ì´ê³  ì•„ì´ë””ì–´ê°€ ë„˜ì¹˜ëŠ” ì• ìì¼ í”„ë¡œë•íŠ¸ ì˜¤ë„ˆ(Agile Product Owner)ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì£¼ëœ ì„ë¬´ëŠ” ì™„ë²½í•œ ê³„íšì´ ì•„ë‹ˆë¼, ê³ ê°ì˜ ìš”ì²­ ì‚¬í•­ì—ì„œ ë‚˜ì˜¨ ê°€ëŠ¥ì„±ì„ ë¹ ì§ì—†ì´ í¬ì°©í•˜ì—¬ ì‚¬ìš©ì ìŠ¤í† ë¦¬ì˜ "ì´ˆì•ˆ" ëª©ë¡ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]
    ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ë°”íƒ•ìœ¼ë¡œ, í”„ë¡œì íŠ¸ì˜ ì „ì²´ì ì¸ ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ì—í”½(Epic) 1ê°œì™€, 
    ê·¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê¸°ëŠ¥ ì•„ì´ë””ì–´ë¥¼ ë‹´ì€ ìœ ì € ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡(User Story Drafts)ì„ ìƒì„±í•˜ë¼.


    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    - ìµœìƒìœ„ì—ëŠ” epicê³¼ user_stories_draft ë‘ ê°œì˜ í‚¤ê°€ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
    - ê° ìœ ì € ìŠ¤í† ë¦¬ëŠ” ê³ ìœ í•œ idì™€ í•¨ê»˜, í‘œì¤€ í˜•ì‹ì¸ "As a..., I want to..., so that..." êµ¬ì¡°ë¥¼ ë”°ë¼ as_a, i_want_to, so_that í‚¤ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    """
    
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì ìš”ì²­ì‚¬í•­: {user_request}")
    ])
    
    chain = prompt | llm.with_structured_output(UserStoriesResult)
    
    result = chain.invoke({
        "user_request": state["user_request"]
    })
    print("ì‘ì„±ëœ ìœ ì €ìŠ¤í† ë¦¬ -----------------")
    print(result)
    print("--------------------------------")

    return {
        "raw_user_stories": result
    }

def generate_non_functional_requirements(state: DecomposeAgentState):
    print("--- ğŸ“ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ ìƒì„± ì¤‘... ---")
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ìˆ˜ë§ì€ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µìœ¼ë¡œ ì´ëˆ 20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì• ìì¼ í”„ë¡œë•íŠ¸ ì˜¤ë„ˆì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì£¼ì–´ì§„ ìœ ì € ìŠ¤í† ë¦¬ ëª©ë¡ì„ ë¶„ì„í•˜ì—¬, í”„ë¡œì íŠ¸ì˜ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]
    ì£¼ì–´ì§€ëŠ” ìœ ì € ìŠ¤í† ë¦¬ ëª©ë¡ì„ ê²€í† í•˜ê³  ë‹¤ìŒ [non_functional_requirements í•„ë“œ]ì™€ [scope_and_constraints í•„ë“œ]ì— ë”°ë¼ ìˆ˜ì • ë° ê°œì„ í•˜ì—¬ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ì„ ìƒì„±í•˜ë¼.

    [non_functional_requirements í•„ë“œ]
    í”„ë¡œì íŠ¸ ì „ì²´ì— ì ìš©ë˜ì–´ì•¼ í•˜ëŠ” ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ì„ ë¬¸ìì—´ ëª©ë¡ìœ¼ë¡œ ì—¬ê¸°ì— ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    ë³´ì•ˆ(ë¹„ë°€ë²ˆí˜¸ ì•”í˜¸í™”), ì„±ëŠ¥(ì‘ë‹µ ì‹œê°„ ëª©í‘œ), ì•ˆì •ì„±, ë¡œê¹… ë“± ì¼ë°˜ì ìœ¼ë¡œ ëˆ„ë½ë˜ê¸° ì‰¬ìš´ í•­ëª©ë“¤ì„ ë°˜ë“œì‹œ ì ê²€í•˜ê³  í¬í•¨ì‹œí‚¤ì‹­ì‹œì˜¤.
    
    [scope_and_constraints í•„ë“œ]
    ì‚¬ìš©ì ìŠ¤í† ë¦¬ì™€ ì „ì²´ ëŒ€í™” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í”„ë¡œì íŠ¸ì˜ ë²”ìœ„(Scope), ê°€ì •(Assumptions), ì œì•½(Constraints)ì„ ëª…í™•íˆ ì‹ë³„í•˜ì‹­ì‹œì˜¤.
    ì‹ë³„ëœ ë‚´ìš©ì„ ê°ê°ì˜ ëª©ë¡ í•„ë“œì— ë‚˜ëˆ„ì–´ ê¸°ìˆ í•˜ì‹­ì‹œì˜¤.

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    """
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì—í”½:\n {epic}\n\nìœ ì € ìŠ¤í† ë¦¬ ëª©ë¡:\n {refined_user_stories}\n\nì‚¬ìš©ìì˜ ìµœì´ˆ ìš”ì²­ ì‚¬í•­:\n {user_request}")
    ])
    chain = prompt | llm.with_structured_output(NonFunctionalRequirements)
    result = chain.invoke({
        "epic": state["epic"],
        "refined_user_stories": state["refined_user_stories"],
        "user_request": state["user_request"]
    })

    print("ì‘ì„±ëœ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ -----------------")
    print(result)
    print("--------------------------------")

    return {
        "non_functional_requirements": result
    }


def refine_user_stories(state: DecomposeAgentState):
    print("--- ğŸ“ ìœ ì € ìŠ¤í† ë¦¬ ì •ì œ ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ìˆ˜ë§ì€ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µìœ¼ë¡œ ì´ëˆ 20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì• ìì¼ í”„ë¡œë•íŠ¸ ì˜¤ë„ˆì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ íŠ¹ê¸°ëŠ” ì£¼ë‹ˆì–´ íŒ€ì›ì´ ë§Œë“  ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆì„ ë‚ ì¹´ë¡­ê²Œ ë¶„ì„í•˜ì—¬, ëª…í™•í•˜ê³ , ê°€ì¹˜ ìˆìœ¼ë©°, ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í† ë¦¬ë¡œ ì¬íƒ„ìƒì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]  
    ì£¼ì–´ì§€ëŠ” [ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡]ì„ ê²€í† í•˜ê³ , ë‹¤ìŒ [ì •ì œ ê·œì¹™]ì— ë”°ë¼ ìˆ˜ì • ë° ê°œì„ í•˜ì—¬ ì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ëª©ë¡(Refined User Stories)ì„ ìƒì„±í•˜ë¼.

    [ì •ì œ ê·œì¹™]
    - ë³‘í•© (Merge): ì„œë¡œ ì¤‘ë³µë˜ê±°ë‚˜ ì§€ë‚˜ì¹˜ê²Œ ìœ ì‚¬í•œ ìŠ¤í† ë¦¬ê°€ ìˆë‹¤ë©´, í•˜ë‚˜ì˜ ëª…í™•í•œ ìŠ¤í† ë¦¬ë¡œ ë³‘í•©í•˜ë¼.
    - ë¶„í•´ (Decompose): í•˜ë‚˜ì˜ ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì—¬ëŸ¬ ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆë‹¤ë©´(Epicì— ê°€ê¹ë‹¤ë©´), ë…ë¦½ì ìœ¼ë¡œ ê°œë°œ ê°€ëŠ¥í•œ ë” ì‘ì€ ìŠ¤í† ë¦¬ ì—¬ëŸ¬ ê°œë¡œ ë¶„í•´í•˜ë¼.
    - êµ¬ì²´í™” (Specify): "ì‰½ê²Œ", "ë¹ ë¥´ê²Œ", "ì˜"ê³¼ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ì„ ì‚¬ìš©ìê°€ ì²´ê°í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ í–‰ë™ì´ë‚˜ ê²°ê³¼ë¡œ ë°”ê¾¸ë¼. (ì˜ˆ: "ìƒí’ˆì„ ì‰½ê²Œ ì°¾ëŠ”ë‹¤" -> "ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒí’ˆì„ í•„í„°ë§í•œë‹¤")
    - ê°€ì¹˜ ë¶€ì—¬ (Add Value): ëª¨ë“  ìŠ¤í† ë¦¬ê°€ ìµœì¢… ì‚¬ìš©ìì—ê²Œ ëª…í™•í•œ ê°€ì¹˜(so_that)ë¥¼ ì œê³µí•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ë¶ˆë¶„ëª…í•˜ë‹¤ë©´ ê°€ì¹˜ë¥¼ ëª…í™•íˆ í•˜ë¼.

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    - IDëŠ” ìƒˆë¡œìš´ ê³ ìœ í•œ ê°’ìœ¼ë¡œ ë‹¤ì‹œ ë¶€ì—¬í•˜ë¼.
    """

    human_prompt = """
    ì‚¬ìš©ìì˜ ìµœì´ˆ ìš”ì²­ ì‚¬í•­:
    {user_request}
    ì—í”½:
    {epic}
    ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡:
    {user_stories_draft}
    í™•ì •ëœ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­:
    {non_functional_requirements}
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", human_prompt)
    ])
    
    chain = prompt | llm.with_structured_output(UserStoriesResult)
    
    result = chain.invoke({
        "user_request": state["user_request"],
        "epic": state["epic"],
        "user_stories_draft": state["raw_user_stories"],
        "non_functional_requirements": state["non_functional_requirements"]
    })

    print("ì •ì œëœ ìœ ì €ìŠ¤í† ë¦¬ -----------------")
    print(result)
    print("--------------------------------")

    return {
        "epic": result.epic,
        "refined_user_stories": result.user_stories_draft
    }


def split_by_group(state: DecomposeAgentState):
    print("--- ğŸ“ ìœ ì € ìŠ¤í† ë¦¬ ê·¸ë£¹í™” ì¤‘... ---")  
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ìˆ˜ë§ì€ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µìœ¼ë¡œ ì´ëˆ 20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì• ìì¼ í”„ë¡œë•íŠ¸ ì˜¤ë„ˆì…ë‹ˆë‹¤. 
    
    [ì§€ì‹œ]  
    ì£¼ì–´ì§€ëŠ” ìœ ì € ìŠ¤í† ë¦¬ ëª©ë¡ì„ ê²€í† í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ ë¶€ì—¬í•˜ê³ , ë‹¤ìŒ [ìš°ì„ ìˆœìœ„ ë¶€ì—¬ ê·œì¹™]ê³¼ [ê·¸ë£¹í™” ê·œì¹™]ì— ë”°ë¼ ìˆ˜ì • ë° ê°œì„ í•˜ì—¬ ê·¸ë£¹í™”ëœ ìœ ì € ìŠ¤í† ë¦¬ ëª©ë¡ì„ ìƒì„±í•˜ë¼.

    [ìš°ì„  ìˆœìœ„ ë¶€ì—¬ ê·œì¹™]
    - ìš°ì„ ìˆœìœ„ ë¶€ì—¬ (Prioritize): ê° ìŠ¤í† ë¦¬ê°€ í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ì„±ê³µ(MVP)ì— ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ íŒë‹¨í•˜ì—¬ priority (High, Medium, Low)ë¥¼ ë¶€ì—¬í•˜ë¼.

    [ê·¸ë£¹í™” ê·œì¹™]
    - ìœ ì‚¬í•˜ê±°ë‚˜ ì—°ê´€ëœ ìŠ¤í† ë¦¬ëŠ” ê·¸ë£¹í™”í•˜ì—¬ ê·¸ë£¹í™”ëœ ìŠ¤í† ë¦¬ ëª©ë¡ìœ¼ë¡œ ìƒì„±í•˜ë¼.
    - ê·¸ë£¹ì€ 1-3ê°œì˜ ìŠ¤í† ë¦¬ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•œë‹¤.
    - 1ê°œì˜ ìŠ¤í† ë¦¬ë¡œ ì´ë¤„ì§„ ê·¸ë£¹ì˜ ìƒì„±ì„ ìµœëŒ€í•œ í”¼í•˜ë¼.

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    - ê·¸ë£¹í™”ëœ ìŠ¤í† ë¦¬ ëª©ë¡ì€ ê·¸ë£¹í™”ëœ ìŠ¤í† ë¦¬ ëª©ë¡ìœ¼ë¡œ ìƒì„±í•˜ë¼.
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì—í”½:\n {epic}\n\nì •ì œëœ ìœ ì € ìŠ¤í† ë¦¬ ëª©ë¡:\n {refined_user_stories}")
    ])
    
    chain = prompt | llm.with_structured_output(RefinedUserStoriesResult)
    
    result = chain.invoke({
        "epic": state["epic"],
        "refined_user_stories": state["refined_user_stories"]
    })
    
    return {
        "refined_user_stories_grouped": result.refined_user_stories
    }


async def generate_final_specifications(state: DecomposeAgentState):
    print("--- ğŸ“ ìœ ì € ìŠ¤í† ë¦¬ ìƒì„¸í™” ì¤‘... ---")
    
    LIMIT = 5
    semaphore = asyncio.Semaphore(LIMIT)
    project_brief = f"ì—í”½: {state['epic']}\n ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­: {state['non_functional_requirements'].non_functional_requirements}\n ë²”ìœ„, ê°€ì •, ì œì•½: {state['non_functional_requirements'].scope_and_constraints}"
    tasks = [llm_call(i, semaphore, user_story_group, project_brief) for i, user_story_group in enumerate(state["refined_user_stories_grouped"])]

    
    results = await asyncio.gather(*tasks)

    flattened_stories = [item for sublist in results for item in sublist]

    final_result = {
        "final_user_stories": results
    }

    print("ìµœì¢… ê²°ê³¼ë¬¼ -----------------")
    for story in flattened_stories:
        print(story.model_dump_json(indent=2))
    print("--------------------------------")

    return {
        "final_specifications": final_result
    }

async def llm_call(
    task_id: int, 
    semaphore: asyncio.Semaphore, 
    user_story_group: List[RefinedUserStoriesDraft], 
    project_brief: str
    ):
    async with semaphore:
        print(f"Task {task_id} started")

        llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
        system_prompt = """
        [ì—­í• ]
        ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ì´ì ê°œë°œ ë¦¬ë”ì…ë‹ˆë‹¤. 
        ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” í”„ë¡œë•íŠ¸ ì˜¤ë„ˆê°€ ì‘ì„±í•œ ì‚¬ìš©ì ìŠ¤í† ë¦¬ë¥¼ ë³´ê³ , ê°œë°œìê°€ ì¦‰ì‹œ ì½”ë”©ì„ ì‹œì‘í•˜ê³  QA ì—”ì§€ë‹ˆì–´ê°€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆëŠ”, 
        ì™„ë²½í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê¸°ìˆ  ëª…ì„¸ì„œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

        [í•µì‹¬ ì§€ì¹¨]
        ì´ ëª…ì„¸ì„œëŠ” í•œ ë²ˆ í™•ì •ë˜ë©´ ë‹¤ì‹œ ìˆ˜ì •ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì „ì œ í•˜ì— ì‘ì„±ë©ë‹ˆë‹¤. 
        ë”°ë¼ì„œ ì¶”í›„ì— í•´ì„ì˜ ì—¬ì§€ê°€ ìƒê¸°ì§€ ì•Šë„ë¡, ëª¨ë“  í•­ëª©ì„ ìµœëŒ€í•œ ì •í™•í•˜ê³  ì„¸ì„¸í•˜ê²Œ ê¸°ìˆ í•´ì•¼ í•˜ë©° í™•ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        ë‹¹ì‹ ì˜ ìœ ì¼í•œ ëª©í‘œëŠ” ì œê³µëœ Pydantic ëª¨ë¸ì˜ ëª¨ë“  í•„ë“œë¥¼ ì•„ë˜ ê·œì¹™ì— ë§ê²Œ ì •í™•í•˜ê²Œ ì±„ìš°ëŠ” ê²ƒì…ë‹ˆë‹¤.

        [ì§€ì‹œ ë° ì‘ì„± ê·œì¹™]
        ì£¼ì–´ì§„ **[ì…ë ¥ ë°ì´í„°]**ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ê° ì‚¬ìš©ì ìŠ¤í† ë¦¬ì— ëŒ€í•´ ì•„ë˜ì˜ detailed_specificationê³¼ acceptance_criteria í•„ë“œë¥¼ ì±„ì›Œë¼.

        1. detailed_specification í•„ë“œ ì‘ì„± ê·œì¹™:
        ëª©í‘œ: ì´ ìŠ¤í† ë¦¬ë¥¼ êµ¬í˜„í•˜ëŠ” ë° í•„ìš”í•œ ëª¨ë“  ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ì„ ìƒì„¸íˆ ê¸°ìˆ í•˜ë¼.

        í¬í•¨í•  ë‚´ìš©:
        ë°ì´í„° ëª¨ë¸: ì´ ê¸°ëŠ¥ì„ ìœ„í•´ í•„ìš”í•œ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ë˜ëŠ” ê°ì²´ ëª¨ë¸ì„ ì •ì˜í•˜ë¼. (í•„ë“œëª…, íƒ€ì…, ì œì•½ì¡°ê±´ ë“±)
        UI/UX ë™ì‘: ì‚¬ìš©ìê°€ ë³´ê²Œ ë  í™”ë©´ì˜ êµ¬ì„± ìš”ì†Œì™€ êµ¬ì²´ì ì¸ ìƒí˜¸ì‘ìš© ë°©ì‹ì„ ì„¤ëª…í•˜ë¼.
        ìœ íš¨ì„± ê²€ì‚¬ ê·œì¹™: ì„œë²„ì™€ í´ë¼ì´ì–¸íŠ¸ ì–‘ìª½ì—ì„œ ìˆ˜í–‰ë˜ì–´ì•¼ í•  ëª¨ë“  ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ê·œì¹™ì„ ëª…ì‹œí•˜ë¼. (í•„ìˆ˜ ì—¬ë¶€, ê¸¸ì´ ì œí•œ, í˜•ì‹ ë“±)
        ê¸°ëŠ¥ ë¡œì§: ê¸°ëŠ¥ì´ ì–´ë–¤ ìˆœì„œë¡œ, ì–´ë–¤ ì¡°ê±´ì— ë”°ë¼ ë™ì‘í•´ì•¼ í•˜ëŠ”ì§€ ë…¼ë¦¬ì ì¸ íë¦„ì„ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.

        2. acceptance_criteria í•„ë“œ ì‘ì„± ê·œì¹™:
        ëª©í‘œ: ì´ ìŠ¤í† ë¦¬ê°€ 'ì™„ë£Œ'ë˜ì—ˆìŒì„ ê°ê´€ì ìœ¼ë¡œ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ ê°œì˜ ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ List[AcceptanceCriteria] í˜•íƒœë¡œ ì‘ì„±í•˜ë¼.
        í¬í•¨í•  ë‚´ìš©:
        ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤: ê° ìŠ¤í† ë¦¬ì— ëŒ€í•´ **ìµœì†Œ 1ê°œì˜ ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤("Happy Path")**ì™€ **1ê°œ ì´ìƒì˜ ì‹¤íŒ¨ ë˜ëŠ” ì—£ì§€ ì¼€ì´ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤("Sad Path")**ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        Gherkin í˜•ì‹: ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ëŠ” ë‹¤ìŒ í˜•ì‹ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.
        scenario: ì‹œë‚˜ë¦¬ì˜¤ì˜ ëª…í™•í•œ ì œëª© (ì˜ˆ: "ì„±ê³µì ì¸ í”„ë¡œì íŠ¸ ë“±ë¡")
        given: ì‹œë‚˜ë¦¬ì˜¤ê°€ ì‹œì‘ë˜ê¸° ì „ì˜ ì „ì œ ì¡°ê±´
        when: ì‚¬ìš©ìê°€ ì·¨í•˜ëŠ” íŠ¹ì • í–‰ë™
        then: ê·¸ í–‰ë™ìœ¼ë¡œ ì¸í•´ ë°œìƒí•´ì•¼ í•˜ëŠ” ê¸°ëŒ€ ê²°ê³¼
        êµ¬ì²´ì„±: "ì‚¬ìš©ì ì •ë³´ê°€ ë³´ì¸ë‹¤"ì™€ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ ëŒ€ì‹ , "í™”ë©´ ìƒë‹¨ì— ì‚¬ìš©ìì˜ ì´ë¦„ê³¼ ì´ë©”ì¼ ì£¼ì†Œê°€ í‘œì‹œëœë‹¤"ì²˜ëŸ¼ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•˜ê²Œ ì‘ì„±í•˜ë¼.

        [ì—­í•  ê²½ê³„]
        ì´ ëª…ì„¸ì„œëŠ” 'ë¬´ì—‡ì„(What)' ë§Œë“¤ì§€ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤. 'ì–´ë–»ê²Œ(How)' ë§Œë“¤ì§€ì— í•´ë‹¹í•˜ëŠ” íŠ¹ì • ê¸°ìˆ  ìŠ¤íƒ(ì˜ˆ: React, Django)ì´ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ë¦„ì€ ì ˆëŒ€ë¡œ ëª…ì‹œí•˜ì§€ ë§ˆë¼.

        [ì‚°ì¶œë¬¼ ê¸°ì¤€]
        ìµœì¢… ì‚°ì¶œë¬¼ì€ ì…ë ¥ë°›ì€ user_story_groupì˜ ê° ìŠ¤í† ë¦¬ ê°ì²´ì— detailed_specificationê³¼ acceptance_criteria í•„ë“œê°€ ì™„ë²½í•˜ê²Œ ì±„ì›Œì§„ Pydantic ëª¨ë¸ í˜•ì‹ì˜ JSONì´ì–´ì•¼ í•œë‹¤.
        """
        
        prompt = ChatPromptTemplate([
            ("system", system_prompt),  
            ("human", "í”„ë¡œì íŠ¸ ë¸Œë¦¬í”„: {project_brief}\nì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ê·¸ë£¹:\n {user_story_group}")
        ])

        chain = prompt | llm.with_structured_output(FinalUserStoriesResult)

        result = await chain.ainvoke({
            "user_story_group": user_story_group,
            "project_brief": project_brief
        })
        print(f"Task {task_id} completed-------")
        return result.final_user_stories

async def main(user_request: str):
    load_dotenv()

    workflow = StateGraph(DecomposeAgentState)

    workflow.add_node("decompose_into_user_stories", decompose_into_user_stories)
    workflow.add_node("generate_non_functional_requirements", generate_non_functional_requirements)
    workflow.add_node("refine_user_stories", refine_user_stories)
    workflow.add_node("split_by_group", split_by_group)
    workflow.add_node("generate_final_specifications", generate_final_specifications)

    workflow.add_edge(START, "decompose_into_user_stories")
    workflow.add_edge("decompose_into_user_stories", "generate_non_functional_requirements")
    workflow.add_edge("generate_non_functional_requirements", "refine_user_stories")
    workflow.add_edge("refine_user_stories", "split_by_group")
    workflow.add_edge("split_by_group", "generate_final_specifications")
    workflow.add_edge("generate_final_specifications", END)

    app = workflow.compile()

    initial_state = {
        "user_request": user_request,
        "epic": None,
        "raw_user_stories": None,
        "refined_user_stories": None,
        "refined_user_stories_grouped": None,
        "non_functional_requirements": None,
        "final_specifications": None
    }

    final_state = await app.ainvoke(initial_state)

    return final_state["final_specifications"]

