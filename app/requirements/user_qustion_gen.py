import os
import json
from dotenv import load_dotenv
import operator
from typing import TypedDict, Annotated, List, Optional
from pydantic import BaseModel, Field
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, START, END
from langchain_core.output_parsers import JsonOutputParser

class ClarifyingQuestionsResult(BaseModel):
    clarifying_questions: List[str] = Field(description="ì¶”ê°€ ì§ˆë¬¸ ëª©ë¡")
    
class UserResponseResult(BaseModel):
    ask_question: str = Field(description="ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€ í…ìŠ¤íŠ¸. ë” ì´ìƒ ì§ˆë¬¸ì´ ì—†ë‹¤ë©´, ëŒ€í™”ê°€ ê³§ ì™„ë£Œë  ê²ƒì„ì„ ì•Œë¦¬ëŠ” ë©”ì‹œì§€ë¥¼ ë‹´ì•„ì£¼ì„¸ìš”.")
    is_complete: bool = Field(description="ì‚¬ìš©ìì˜ ì‘ë‹µì´ ì¶©ë¶„í•œì§€ íŒë‹¨í•˜ë¼. ì¶©ë¶„í•˜ë©´ true, ì•„ë‹ˆë©´ false")

class AgentState(TypedDict):
    request: str
    interaction_mode: str
    question_count: int
    ask_question : str
    result: str
    messages: Annotated[list, operator.add]
    is_complete: bool

def generate_next_question_or_complete(state: AgentState):
    """ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì„ ìƒì„±í•˜ê±°ë‚˜, ëŒ€í™”ë¥¼ ì™„ë£Œí• ì§€ ê²°ì •í•˜ëŠ” ë…¸ë“œ"""
    print("--- ğŸ¤” ë‹¤ìŒ í–‰ë™ ê²°ì • ì¤‘... ---")

    state['question_count'] += 1
    max_questions = 3 if state['interaction_mode'] == 'ë¹ ë¥¸' else 7

    if state['question_count'] > max_questions:
        print(f"--- ğŸ’¬ ìµœëŒ€ ì§ˆë¬¸ ê°œìˆ˜({max_questions}ê°œ)ì— ë„ë‹¬í•˜ì—¬ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ---")
        return {
            "is_complete": True,
            "ask_question": "ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆ˜ì§‘ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì´ì œ ìµœì¢… ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œë¥¼ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤."
        }

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    
    system_prompt = """
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ë¥¼ êµ¬ì²´í™”í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” ë›°ì–´ë‚œ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ í”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­ì„ ëª…í™•íˆ í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œë°œíŒ€ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìƒì„¸ ëª…ì„¸ì„œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ìƒí™©]
    ì§€ê¸ˆê¹Œì§€ ì‚¬ìš©ìì™€ ë‚˜ëˆˆ ëŒ€í™” ê¸°ë¡ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤. ì´ ëŒ€í™” ê¸°ë¡ì„ ë©´ë°€íˆ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    ì‚¬ìš©ìëŠ” '{interaction_mode}' ëª¨ë“œë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë“œì— ë§ì¶° ë‹¹ì‹ ì˜ ì§ˆë¬¸ ìŠ¤íƒ€ì¼ê³¼ ê¹Šì´ë¥¼ ì¡°ì ˆí•´ì•¼ í•©ë‹ˆë‹¤.
    í˜„ì¬ ì§ˆë¬¸ì€ {question_count}ë²ˆì§¸ ì§ˆë¬¸ì´ë©°, ìµœëŒ€ {max_questions}ê°œê¹Œì§€ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    [ì‘ì—…]
    ë‹¤ìŒ ë‘ ê°€ì§€ í–‰ë™ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³ , ê·¸ì— ë§ëŠ” ê²°ê³¼(JSON í˜•ì‹)ë¥¼ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

    1. ì¶”ê°€ ì§ˆë¬¸í•˜ê¸° (is_complete: false):
       - ì•„ì§ ìš”êµ¬ì‚¬í•­ì´ ë¶ˆë¶„ëª…í•˜ê±°ë‚˜ ë” êµ¬ì²´í™”í•  í•„ìš”ê°€ ìˆë‹¤ê³  íŒë‹¨ë  ë•Œ ì„ íƒí•©ë‹ˆë‹¤.
       - ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê³  ê¶ê¸ˆí•œ **ë‹¨ í•˜ë‚˜ì˜ ì§ˆë¬¸**ì„ ìƒì„±í•©ë‹ˆë‹¤.
       - **(ì¤‘ìš”)** ì‚¬ìš©ìì˜ ë‹µë³€ ë¶€ë‹´ì„ ì¤„ì´ê¸° ìœ„í•´, ê°€ëŠ¥í•œ ê²½ìš° **ì ì ˆí•œ ê¸°ë³¸ê°’ì„ í¬í•¨í•œ ì œì•ˆ í˜•íƒœ**ë¡œ ì§ˆë¬¸ì„ êµ¬ì„±í•˜ì„¸ìš”.
         - ì˜ˆì‹œ (ë‚˜ìœ ì§ˆë¬¸): "ì–´ë–¤ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì‹œê² ì–´ìš”?"
         - ì˜ˆì‹œ (ì¢‹ì€ ì§ˆë¬¸): "ë°ì´í„°ë² ì´ìŠ¤ëŠ” í‘œì¤€ì ì¸ PostgreSQLë¡œ êµ¬ì„±í•˜ëŠ” ê²ƒì„ ì œì•ˆí•˜ëŠ”ë°, ê´œì°®ìœ¼ì‹ ê°€ìš”? ë‹¤ë¥¸ ì„ í˜¸í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”."
       - '{interaction_mode}' ëª¨ë“œì— ë”°ë¼ ì§ˆë¬¸ì˜ ê¹Šì´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.
         - 'ë¹ ë¥¸' ëª¨ë“œ: í•µì‹¬ ê¸°ëŠ¥ê³¼ ë²”ìœ„ì— ì§‘ì¤‘ëœ ìµœì†Œí•œì˜ ì§ˆë¬¸ì„ í•©ë‹ˆë‹¤.
         - 'ìƒì„¸' ëª¨ë“œ: ê¸°ìˆ  ìŠ¤íƒ, ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­, ì—£ì§€ ì¼€ì´ìŠ¤ ë“± ë” ê¹Šì´ ìˆëŠ” ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    2. ëŒ€í™” ì™„ë£Œí•˜ê¸° (is_complete: true):
       - ì‚¬ìš©ìê°€ "ê·¸ë§Œ", "ì™„ë£Œ", "ì¶©ë¶„í•´ìš”" ë“± ëŒ€í™” ì¢…ë£Œë¥¼ ì˜ë¯¸í•˜ëŠ” ë°œì–¸ì„ í–ˆì„ ê²½ìš°, ì¦‰ì‹œ ì´ í–‰ë™ì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.
       - ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì´ ì¶©ë¶„íˆ ëª…í™•í•´ì ¸ì„œ ë” ì´ìƒ ì§ˆë¬¸í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ë  ë•Œ ì„ íƒí•©ë‹ˆë‹¤.
       - 'ë¹ ë¥¸' ëª¨ë“œì—ì„œëŠ” ëª‡ ê°€ì§€ í•µì‹¬ ì‚¬í•­ë§Œ í™•ì¸ë˜ë©´ ë¹ ë¥´ê²Œ ì™„ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
       - 'ìƒì„¸' ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ì¸¡ë©´ì´ ì¶©ë¶„íˆ ë‹¤ë£¨ì–´ì¡ŒëŠ”ì§€ ì‹ ì¤‘í•˜ê²Œ íŒë‹¨ í›„ ì™„ë£Œí•©ë‹ˆë‹¤.
       - is_completeë¥¼ trueë¡œ ì„¤ì •í•˜ê³  ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.

    [ê·œì¹™]
    - ë°˜ë“œì‹œ ì§€ì •ëœ Pydantic ëª¨ë¸ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    - ëŒ€í™” ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ë§¥ë½ì— ë§ëŠ” ì§ˆë¬¸ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    """

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
    ])

    chain = prompt | llm.with_structured_output(UserResponseResult)

    result = chain.invoke({
        "messages": state["messages"],
        "interaction_mode": state["interaction_mode"],
        "question_count": state["question_count"],
        "max_questions": max_questions
    })

    print(f"---\n ğŸ—£ï¸ ì§ˆë¬¸ {state['question_count']}: {result.ask_question} (ì™„ë£Œ: {result.is_complete}) \n---\n")

    return {
        "ask_question": result.ask_question, 
        "is_complete": result.is_complete or False, 
        "question_count": state["question_count"],
        "messages": [AIMessage(content=result.ask_question)]
    }


def user_response(state: AgentState):
    """ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë°›ëŠ” ë…¸ë“œ"""
    print(state.get("ask_question", "ì§ˆë¬¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìš”êµ¬ì‚¬í•­ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”."))
    response = input("ë‹µë³€: ")
    return {"messages": [HumanMessage(content=response)]}  

def final_result_generation(state: AgentState):
    """ìµœì¢… ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    print("--- ğŸ“ ìµœì¢… ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ëŠ” ì¤‘... ---")
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    ë‹¹ì‹ ì€ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™”ë‚´ì—­ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ëŠ” ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. 
    ìµœì¢… ê²°ê³¼ë¬¼ì€ í”„ë¡œì íŠ¸ì˜ ë²”ìœ„, í•µì‹¬ ê¸°ëŠ¥, ì œì•½ ì¡°ê±´ì„ ëª…í™•íˆ í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤

    [ìµœì¢… ê²°ê³¼ë¬¼ ìƒì„± ê·œì¹™]
    - ìµœì¢… ê²°ê³¼ë¬¼ì€ í”„ë¡œì íŠ¸ì˜ ë²”ìœ„, í•µì‹¬ ê¸°ëŠ¥, ì œì•½ ì¡°ê±´ì„ ëª…í™•íˆ í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤
    - ìµœì¢… ê²°ê³¼ë¬¼ì€ 1. ì£¼ì œ 2. ê¸°ëŠ¥ 3. ë²”ìœ„, ê°€ì •, ì œì•½ 4. ê¸°íƒ€ ì •ë³´ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ìµœì¢… ê²°ê³¼ë¬¼ì€ ì‚¬ìš©ìì™€ì˜ ëŒ€í™” ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ì–¸ê¸‰ë˜ê±°ë‚˜ ì•”ì‹œëœ ëª¨ë“  ê¸°ëŠ¥ì— ëŒ€í•´ ì‘ì„±í•˜ë¼.
    - ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ì‘ì„±í•˜ë¼.
    - ìµœì¢… ê²°ê³¼ë¬¼ì€ ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - ìµœì¢… ê²°ê³¼ë¬¼ ì™¸ì˜ ë„ˆì˜ ìƒê°ì´ë‚˜ ëŒ€í™” ì´ë ¥ë“±ì˜ ëª¨ë“  ê¸°íƒ€ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤.
    """
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
        ("human", "ëŒ€í™” ë‚´ì—­ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ë¬¼ì„ ìƒì„±í•˜ë¼.")
    ])
    chain = prompt | llm
    result = chain.invoke({
        "messages": state["messages"]
    })

    print("ìµœì¢… ê²°ê³¼ë¬¼ -----------------")
    print(result.content)
    print("--------------------------------")

    return {"result": result.content}

def is_complete(state: AgentState):
    return state["is_complete"] == True

def main(first_request: str, interaction_mode: str):
    load_dotenv()

    workflow = StateGraph(AgentState)

    workflow.add_node("generate_next_question", generate_next_question_or_complete)
    workflow.add_node("user_response", user_response)
    workflow.add_node("final_result_generation", final_result_generation)

    workflow.add_edge(START, "generate_next_question")
    workflow.add_conditional_edges("generate_next_question", is_complete, {True: "final_result_generation", False: "user_response"})
    workflow.add_edge("user_response", "generate_next_question")
    workflow.add_edge("final_result_generation", END)

    app = workflow.compile()

    initial_state = {
        "request": first_request,
        "interaction_mode": interaction_mode,
        "messages": [HumanMessage(content=first_request)],
        "is_complete": False,
        "ask_question": "",
        "result": "",
        "question_count": 0
    }

    final_state = app.invoke(initial_state)

    return final_state["result"]

if __name__ == "__main__":
    print("ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–¤ í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê³  ì‹¶ìœ¼ì‹ ê°€ìš”? ìì„¸íˆ ì•Œë ¤ì£¼ì‹¤ìˆ˜ë¡ ì¢‹ìŠµë‹ˆë‹¤.")
    first_request = input("ìš”êµ¬ì‚¬í•­: ")
    
    mode = ""
    while mode not in ["1", "2"]:
        print("\nì–´ë–¤ ëª¨ë“œë¡œ ì§„í–‰í• ê¹Œìš”?")
        print("1. ë¹ ë¥¸ ëª¨ë“œ (í•µì‹¬ ì§ˆë¬¸ ìœ„ì£¼ë¡œ ë¹ ë¥´ê²Œ ì§„í–‰)")
        print("2. ìƒì„¸ ëª¨ë“œ (ê¸°ìˆ  ìŠ¤íƒ, ì œì•½ ì¡°ê±´ ë“± ìƒì„¸í•˜ê²Œ ì§„í–‰)")
        mode = input("ì„ íƒ (1 ë˜ëŠ” 2): ")

    interaction_mode = "ë¹ ë¥¸" if mode == "1" else "ìƒì„¸"
    
    print(main(first_request, interaction_mode))