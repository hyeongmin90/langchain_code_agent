import os
import json
from dotenv import load_dotenv
import operator
from typing import TypedDict, Annotated, List, Optional
from pydantic import BaseModel, Field
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, START, END
from schemas import FinalUserStoriesResult, AnalysisResult, FeedbackAnalysisResult, AnalysisAgentState

def request_analysis(state: AnalysisAgentState):
    """ë¶„ì„ëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ë¥¼ ë¶„ì„í•˜ëŠ” ë…¸ë“œ"""
    print("--- ğŸ“ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³  ìš”êµ¬ì‚¬í•­ì„ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ ì •ë¦¬í•˜ë¼.
    í•œë²ˆ í™•ì •ëœ ìš”êµ¬ì‚¬í•­ì€ ë‹¤ì‹œ ìˆ˜ì •í•˜ì§€ ì•Šê³  êµ¬í˜„í•˜ê¸° ë•Œë¬¸ì— ì¶”í›„ì— ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì§€ ì•Šë„ë¡ ìµœëŒ€í•œ ì •í™•í•˜ê³  ì„¸ì„¸í•˜ê²Œ ì‘ì„±í•´ì•¼í•œë‹¤.
    ëª¨ë“  ê¸°ëŠ¥ì€ í™•ì •ë˜ì–´ì•¼ í•œë‹¤.

    [ëª©í‘œ]
    - ê³ ê°ì˜ ì¶”ìƒì  ìš”êµ¬ë¥¼ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ì •ì œí•œë‹¤.
    - ê¸°ëŠ¥ì /ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ë¥¼ ëª…í™•íˆ ë¶„ë¦¬í•˜ê³ , ëª¨í˜¸ì„±ì„ ì œê±°í•œë‹¤.
    - ì •ë§ í•„ìš”í•œ ì •ë³´ì— ëŒ€í•´ì„œë§Œ ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ë³´ì™„í•œë‹¤.

    [ì‘ì„± ê·œì¹™]
    - ê¸°ëŠ¥/ë¹„ê¸°ëŠ¥ ìš”êµ¬ëŠ” ë²ˆí˜¸ ëª©ë¡ í˜•íƒœë¡œ ì‘ì„±í•œë‹¤.
    - ê¸°ëŠ¥ ìš”êµ¬ëŠ” ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ì‘ì„±í•˜ê³ , ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ëŠ” ìµœì†Œí•œì˜ ê¸°ì¤€ì„ ê°€ì§€ê³  ì‘ì„±í•œë‹¤.
    - ëª¨í˜¸í•œ í‘œí˜„(ë¹ ë¥´ê²Œ, í¬ê²Œ, ì•ˆì •ì  ë“±)ì€ ê¸ˆì§€í•˜ê³ , êµ¬ì²´ì  ìˆ˜ì¹˜/ì¡°ê±´/ì‚¬ë¡€ë¡œ ëŒ€ì²´í•œë‹¤.
    - ë²”ìœ„(Scope), ê°€ì •(Assumptions), ì œì•½(Constraints)ì´ ì•”ì‹œë˜ì–´ ìˆìœ¼ë©´ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ê³  í•´ë‹¹ í•­ëª©ì— í†µí•©í•œë‹¤.
    - ì‚¬ìš©ìì˜ ìš”ì²­ì´ ì—†ì„ ê²½ìš°ì—ëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì„ ìš°ì„  ì±„íƒí•˜ì—¬ ì‘ì„±í•˜ë©°, ë¦¬ìŠ¤í¬ë¥¼ ìµœì†Œí™”í•œë‹¤.
    - ë³´ì•ˆ/ê°œì¸ì •ë³´, ë¡œê¹…/ëª¨ë‹ˆí„°ë§, ë°°í¬/ë¡¤ë°±, êµ­ì œí™”/í˜„ì§€í™”, ì ‘ê·¼ì„± ë“± ì¼ë°˜ì ìœ¼ë¡œ ëˆ„ë½ë˜ê¸° ì‰¬ìš´ ë¹„ê¸°ëŠ¥ í•­ëª©ì„ ìŠµê´€ì ìœ¼ë¡œ ì ê²€í•œë‹¤.
    - í”¼ë“œë°±ì´ ìˆë‹¤ë©´ ì´ë¥¼ ë°˜ì˜í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì„ ìˆ˜ì •í•˜ë¼.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - Pydantic ëª¨ë¸ì˜ í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.

    í˜„ì¬ ê³„íšëœ ê¸°ëŠ¥: {functional_requirements}

    í”¼ë“œë°±: {feedback}
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬: {user_stories}")
    ])

    chain = prompt | llm.with_structured_output(AnalysisResult)

    result = chain.invoke({
        "feedback": state.get("feedback", ""), 
        "functional_requirements": state.get("functional_requirements", "ì•„ì§ ì •ì˜ë˜ì§€ ì•ŠìŒ"),
        "user_stories": state["user_stories"]
    })

    return {
        "goal": result.goal,
        "functional_requirements": result.functional_requirements, 
    }

def feedback_analysis(state: AnalysisAgentState):
    """ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€í•˜ëŠ” ë…¸ë“œ"""

    print("--- ğŸ“ ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€ ì¤‘... ---")
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
   
    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ìœ ì €ìŠ¤í† ë¦¬ì— ëŒ€í•´ ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì„ í‰ê°€í•˜ë¼.

    [í‰ê°€ ê·œì¹™]
    - ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì´ ìœ ì €ìŠ¤í† ë¦¬ë¥¼ ì¶©ë¶„íˆ ë°˜ì˜í–ˆëŠ”ì§€ í‰ê°€í•˜ë¼.
    - ì¶©ë¶„íˆ ë°˜ì˜í–ˆë‹¤ë©´ is_completeë¥¼ trueë¡œ ì„¤ì •í•œë‹¤.
    - ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í–ˆë‹¤ë©´ is_completeë¥¼ falseë¡œ ì„¤ì •í•˜ê³  feedbackì— í‰ê°€ë¥¼ ì‘ì„±í•œë‹¤.
    
    ìœ ì €ìŠ¤í† ë¦¬: {user_stories}

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - Pydantic ëª¨ë¸ì˜ í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.

    ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­: {functional_requirements}
    """


    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€í•˜ë¼."),
    ])

    chain = prompt | llm.with_structured_output(FeedbackAnalysisResult)
    result = chain.invoke({
        "functional_requirements": state["functional_requirements"],
        "user_stories": state["user_stories"]
    })

    return {
        "feedback": result.feedback, 
        "is_complete": result.is_complete
    }
    

def is_complete(state: AnalysisAgentState):
    return state["is_complete"]

def main(user_stories: FinalUserStoriesResult):
    load_dotenv()

    workflow = StateGraph(AnalysisAgentState)

    workflow.add_node("request_analysis", request_analysis)
    workflow.add_node("feedback_analysis", feedback_analysis)
    
    workflow.add_edge(START, "request_analysis") 
    workflow.add_edge("request_analysis", "feedback_analysis")
    workflow.add_conditional_edges("feedback_analysis", is_complete,{True: END, False: "request_analysis"})    

    app = workflow.compile()

    initial_state = {
        "user_stories": user_stories,
        "functional_requirements": "",
        "is_complete": False,
        "feedback": None,
    }
    
    final_state = app.invoke(initial_state)
    
    return final_state
    
