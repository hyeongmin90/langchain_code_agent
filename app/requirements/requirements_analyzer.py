import os
import json
from dotenv import load_dotenv
import operator
from typing import TypedDict, Annotated, List, Optional
from pydantic import BaseModel, Field
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, START, END

class AnalysisResult(BaseModel):
    goal: str = Field(description="ë¶„ì„ëœ ê³ ê°ì˜ ìš”ì²­ì‚¬í•­ì— ëŒ€í•œ ëª©í‘œ")
    functional_requirements: str = Field(description="ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­")
    non_functional_requirements: str = Field(description="ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­")

class FeedbackAnalysisResult(BaseModel):
    feedback: str = Field(description="ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ í‰ê°€")
    is_complete: bool = Field(description="ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì´ ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ì¶©ë¶„íˆ ë°˜ì˜í–ˆëŠ”ì§€ í‰ê°€í•˜ë¼. ì¶©ë¶„íˆ ë°˜ì˜í–ˆë‹¤ë©´ true, ì•„ë‹ˆë©´ false")


class AgentState(TypedDict):
    request: str
    functional_requirements: str
    non_functional_requirements: str
    is_complete: bool
    feedback: Optional[str]
    messages: Annotated[list, operator.add]
    clarifying_questions: List[str]


def request_analysis(state: AgentState):
    """ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ë¶„ì„í•˜ê³ , ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” ë…¸ë“œ"""
    print("--- ğŸ“ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³  ìš”êµ¬ì‚¬í•­ì„ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ ì •ë¦¬í•˜ë¼.
    í•œë²ˆ í™•ì •ëœ ìš”êµ¬ì‚¬í•­ì€ ë‹¤ì‹œ ìˆ˜ì •í•˜ì§€ ì•Šê³  êµ¬í˜„í•˜ê¸° ë•Œë¬¸ì— ì¶”í›„ì— ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì§€ ì•Šë„ë¡ ìµœëŒ€í•œ ì •í™•í•˜ê³  ì„¸ì„¸í•˜ê²Œ ì‘ì„±í•´ì•¼í•œë‹¤.
    ëª¨ë“  ê¸°ëŠ¥ì€ í™•ì •ë˜ì–´ì•¼ í•œë‹¤.

    [ëª©í‘œ]
    - ê³ ê°ì˜ ì¶”ìƒì  ìš”êµ¬ë¥¼ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ì •ì œí•œë‹¤.
    - ê¸°ëŠ¥ì /ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ë¥¼ ëª…í™•íˆ ë¶„ë¦¬í•˜ê³ , ëª¨í˜¸ì„±ì„ ì œê±°í•œë‹¤.
    - ì •ë§ í•„ìš”í•œ ì •ë³´ì— ëŒ€í•´ì„œë§Œ ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ë³´ì™„í•œë‹¤.

    [ì‘ì„± ê·œì¹™]
    - ê¸°ëŠ¥/ë¹„ê¸°ëŠ¥ ìš”êµ¬ëŠ” ë²ˆí˜¸ ëª©ë¡ í˜•íƒœë¡œ ì‘ì„±í•œë‹¤.
    - ê¸°ëŠ¥ ìš”êµ¬ëŠ” ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ì‘ì„±í•˜ê³ , ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ëŠ” ìµœì†Œí•œì˜ ê¸°ì¤€ì„ ê°€ì§€ê³  ì‘ì„±í•œë‹¤.
    - ëª¨í˜¸í•œ í‘œí˜„(ë¹ ë¥´ê²Œ, í¬ê²Œ, ì•ˆì •ì  ë“±)ì€ ê¸ˆì§€í•˜ê³ , êµ¬ì²´ì  ìˆ˜ì¹˜/ì¡°ê±´/ì‚¬ë¡€ë¡œ ëŒ€ì²´í•œë‹¤.
    - ë²”ìœ„(Scope), ê°€ì •(Assumptions), ì œì•½(Constraints)ì´ ì•”ì‹œë˜ì–´ ìˆìœ¼ë©´ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ê³  í•´ë‹¹ í•­ëª©ì— í†µí•©í•œë‹¤.
    - ì‚¬ìš©ìì˜ ìš”ì²­ì´ ì—†ì„ ê²½ìš°ì—ëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì„ ìš°ì„  ì±„íƒí•˜ì—¬ ì‘ì„±í•˜ë©°, ë¦¬ìŠ¤í¬ë¥¼ ìµœì†Œí™”í•œë‹¤.
    - ë³´ì•ˆ/ê°œì¸ì •ë³´, ë¡œê¹…/ëª¨ë‹ˆí„°ë§, ë°°í¬/ë¡¤ë°±, êµ­ì œí™”/í˜„ì§€í™”, ì ‘ê·¼ì„± ë“± ì¼ë°˜ì ìœ¼ë¡œ ëˆ„ë½ë˜ê¸° ì‰¬ìš´ ë¹„ê¸°ëŠ¥ í•­ëª©ì„ ìŠµê´€ì ìœ¼ë¡œ ì ê²€í•œë‹¤.
    - í”¼ë“œë°±ì´ ìˆë‹¤ë©´ ì´ë¥¼ ë°˜ì˜í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì„ ìˆ˜ì •í•˜ë¼.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - Pydantic ëª¨ë¸ì˜ í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.


    í˜„ì¬ ê³„íšëœ ê¸°ëŠ¥: {functional_requirements}

    í”¼ë“œë°±: {feedback}
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì ì‘ë‹µ: {request}")
    ])

    chain = prompt | llm.with_structured_output(AnalysisResult)

    result = chain.invoke({
        "request": state["request"],    
        "feedback": state.get("feedback", ""), 
        "functional_requirements": state.get("functional_requirements", "ì•„ì§ ì •ì˜ë˜ì§€ ì•ŠìŒ"),
        "non_functional_requirements": state.get("non_functional_requirements", "ì•„ì§ ì •ì˜ë˜ì§€ ì•ŠìŒ"),
    })

    return {
        "goal": result.goal,
        "functional_requirements": result.functional_requirements, 
        "non_functional_requirements": result.non_functional_requirements
    }

def feedback_analysis(state: AgentState):
    """ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€í•˜ëŠ” ë…¸ë“œ"""

    print("--- ğŸ“ ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€ ì¤‘... ---")
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
   
    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì— ëŒ€í•´ ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì„ í‰ê°€í•˜ë¼.

    [í‰ê°€ ê·œì¹™]
    - ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì´ ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ì¶©ë¶„íˆ ë°˜ì˜í–ˆëŠ”ì§€ í‰ê°€í•˜ë¼.
    - ì¶©ë¶„íˆ ë°˜ì˜í–ˆë‹¤ë©´ is_completeë¥¼ trueë¡œ ì„¤ì •í•œë‹¤.
    - ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í–ˆë‹¤ë©´ is_completeë¥¼ falseë¡œ ì„¤ì •í•˜ê³  feedbackì— í‰ê°€ë¥¼ ì‘ì„±í•œë‹¤.
    
    - ë˜í•œ ì‚¬ìš©ìê°€ ìš”êµ¬ì‚¬í•­ì„ ì¶”í›„ì— ìˆ˜ì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ ê°ì•ˆí•˜ì—¬ í‰ê°€í•˜ë¼.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - Pydantic ëª¨ë¸ì˜ í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.

    ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­: {functional_requirements}
    ì‘ì„±ëœ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­: {non_functional_requirements}
    """

    chat_history = state["messages"]

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€í•˜ë¼."),
    ])

    chain = prompt | llm.with_structured_output(FeedbackAnalysisResult)
    result = chain.invoke({
        "chat_history": chat_history,
        "functional_requirements": state["functional_requirements"],
        "non_functional_requirements": state["non_functional_requirements"],
    })

    return {
        "feedback": result.feedback, 
        "is_complete": result.is_complete
    }
    

def is_complete(state: AgentState):
    return state["is_complete"]

def main(first_request: str):
    load_dotenv()

    workflow = StateGraph(AgentState)

    workflow.add_node("request_analysis", request_analysis)
    workflow.add_node("feedback_analysis", feedback_analysis)
    
    workflow.add_edge(START, "request_analysis") 
    workflow.add_edge("request_analysis", "feedback_analysis")
    workflow.add_conditional_edges("feedback_analysis", is_complete,{True: END, False: "request_analysis"})    

    app = workflow.compile()

    initial_state = {
        "request": first_request,
        "functional_requirements": "",
        "non_functional_requirements": "",
        "is_complete": False,
        "feedback": None,
        "messages": [HumanMessage(content=first_request)]
    }
    
    final_state = app.invoke(initial_state)

    print("\n" + "="*60)
    print("âœ… ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì™„ë£Œ!")
    print("="*60)
    print(f"\nğŸ“‹ ëª©í‘œ: {final_state['request']}")
    print(f"\nğŸ“Œ ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­:\n{final_state['functional_requirements']}")
    print(f"\nğŸ“Œ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­:\n{final_state['non_functional_requirements']}")
    
    return final_state
    

if __name__ == "__main__":
    print("ğŸ’¡ ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”:")
    first_request = input("ğŸ‘‰ ")
    main(first_request)