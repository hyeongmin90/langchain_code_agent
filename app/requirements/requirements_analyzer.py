import os
import json
from dotenv import load_dotenv
import operator
from typing import TypedDict, Annotated, List, Optional
from pydantic import BaseModel, Field
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, START, END

class AnalysisResult(BaseModel):
    goal: str = Field(description="ë¶„ì„ëœ ê³ ê°ì˜ ìš”ì²­ì‚¬í•­ì— ëŒ€í•œ ëª©í‘œ")
    functional_requirements: str = Field(description="ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­")
    non_functional_requirements: str = Field(description="ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­")

class FeedbackAnalysisResult(BaseModel):
    feedback: str = Field(description="ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ í‰ê°€")
    is_complete: bool = Field(description="ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì´ ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ì¶©ë¶„íˆ ë°˜ì˜í–ˆëŠ”ì§€ í‰ê°€í•˜ë¼. ì¶©ë¶„íˆ ë°˜ì˜í–ˆë‹¤ë©´ true, ì•„ë‹ˆë©´ false")

class ClarifyingQuestionsResult(BaseModel):
    clarifying_questions: List[str] = Field(description="ì¶”ê°€ ì§ˆë¬¸ ëª©ë¡")

class UserResponseResult(BaseModel):
    request: str = Field(description="ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€ í…ìŠ¤íŠ¸")

class AgentState(TypedDict):
    request: str
    functional_requirements: str
    non_functional_requirements: str
    is_complete: bool
    feedback: Optional[str]
    messages: Annotated[list, operator.add]
    clarifying_questions: List[str]

def generate_clarifying_questions(state: AgentState):
    """ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì— ëŒ€í•´ ì¶”ê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    print("--- ğŸ“ ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ì¤‘... ---")
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ëª…ì„¸ì„œ ì‘ì„±ì— ì•ì„œ ë°˜ë“œì‹œ í™•ì¸í•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸ 3ê°€ì§€ë¥¼ ìƒì„±í•˜ì„¸ìš”. 
    ì§ˆë¬¸ì€ í”„ë¡œì íŠ¸ì˜ ë²”ìœ„, í•µì‹¬ ê¸°ëŠ¥, ì œì•½ ì¡°ê±´ì„ ëª…í™•íˆ í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶°ì•¼ í•©ë‹ˆë‹¤
    
    [ì¶”ê°€ ì§ˆë¬¸ ìƒì„± ê·œì¹™]
    - ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì— ëŒ€í•´ ì¶”ê°€ ì§ˆë¬¸ì„ ìƒì„±í•˜ë¼.
    - ì¶”ê°€ ì§ˆë¬¸ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì— ëŒ€í•´ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” ì§ˆë¬¸ì´ë‹¤.
    - ì¶”ê°€ ì§ˆë¬¸ì€ ìµœëŒ€ 3ê°€ì§€ê¹Œì§€ ìƒì„±í•˜ë¼.
    - ì¶”ê°€ ì§ˆë¬¸ì€ ëª¨í˜¸í•˜ì§€ ì•Šê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë¼.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆ ì§€ì¹¨ì„ ì—„ê²¹íˆ ë”°ë¥¸ë‹¤. í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.
      - clarifying_questions: ì¶”ê°€ ì§ˆë¬¸ ëª©ë¡.
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì ì‘ë‹µ: {request}")
    ])
    chain = prompt | llm.with_structured_output(ClarifyingQuestionsResult)
    result = chain.invoke({
        "request": state["request"]
    })

    return {
        "clarifying_questions": result.clarifying_questions
    }

def user_response(state: AgentState):
    """ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë°›ëŠ” ë…¸ë“œ"""
    print("--- ğŸ“ ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë°›ëŠ” ì¤‘... ---")
    
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")
    system_prompt = """
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ë¥¼ í˜„ì‹¤ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ AI í”„ë¡œì íŠ¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ë”±ë”±í•œ ì§ˆë¬¸ ëª©ë¡ì„ ë”°ëœ»í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¡œ ë°”ê¾¸ì–´, ì‚¬ìš©ìê°€ í¸ì•ˆí•˜ê²Œ ìì‹ ì˜ ìƒê°ì„ ì´ì•¼ê¸°í•˜ë„ë¡ ë•ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]
    ì•„ë˜ [í•µì‹¬ ì§ˆë¬¸ ëª©ë¡]ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì†Œê°œí•˜ë©´ì„œ, ì‚¬ìš©ìê°€ ë‹µë³€ì„ ì…ë ¥í•˜ë„ë¡ ìœ ë„í•˜ëŠ” í™˜ì˜ ë©”ì‹œì§€ë¥¼ ì‘ì„±í•˜ì‹­ì‹œì˜¤. 
    ì´ ë©”ì‹œì§€ëŠ” ì‚¬ìš©ìê°€ ë³´ê²Œ ë  ì²«ì¸ìƒì´ë¯€ë¡œ, í˜‘ë ¥ì ì¸ ë¶„ìœ„ê¸°ë¥¼ ì¡°ì„±í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

    [í•µì‹¬ ì§ˆë¬¸ ëª©ë¡]
    {clarifying_questions}

    [ì¶œë ¥ í˜•ì‹]
    - ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ìµœì¢… í™˜ì˜ ë©”ì‹œì§€ í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

    [ê·œì¹™]
    - ì™œ ì´ ì§ˆë¬¸ë“¤ì´ í•„ìš”í•œì§€ ê°„ëµíˆ ì„¤ëª…í•˜ì—¬ ì‚¬ìš©ìë¥¼ ì•ˆì‹¬ì‹œí‚¤ì‹­ì‹œì˜¤. (ì˜ˆ: "ë” ì •í™•í•œ ê¸°ëŠ¥ ëª…ì„¸ì„œë¥¼ ë§Œë“¤ê¸° ìœ„í•´...")
    - ë”±ë”±í•œ ì§ˆë¬¸ ì–´ì¡°ê°€ ì•„ë‹Œ, ë¶€ë“œëŸ½ê³  ëŒ€í™”í•˜ëŠ” ë“¯í•œ ë¬¸ì²´ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
    - ë©”ì‹œì§€ ë§ˆì§€ë§‰ì—ëŠ” ì‚¬ìš©ìì˜ ë‹µë³€ì„ ê¸°ë‹¤ë¦°ë‹¤ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ í’ê²¨ ëŒ€í™”ë¥¼ ìœ ë„í•˜ì‹­ì‹œì˜¤.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆ ì§€ì¹¨ì„ ì—„ê²©íˆ ë”°ë¥¸ë‹¤. í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.
      - request: ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë©”ì‹œì§€ í…ìŠ¤íŠ¸.
    """
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì ì‘ë‹µ: {request}"),
    ])
    chain = prompt | llm.with_structured_output(UserResponseResult)

    result = chain.invoke({
        "request": state["request"],
        "clarifying_questions": state["clarifying_questions"]
    })

    return {"request": result.request, "messages": state["messages"]}

def request_analysis(state: AgentState):
    """ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ë¶„ì„í•˜ê³ , ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ëŠ” ë…¸ë“œ"""
    print("--- ğŸ“ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ì•„ë˜ ê·œì¹™ì— ë”°ë¼ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ê³  ìš”êµ¬ì‚¬í•­ì„ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ ì •ë¦¬í•˜ë¼.
    í•œë²ˆ í™•ì •ëœ ìš”êµ¬ì‚¬í•­ì€ ë‹¤ì‹œ ìˆ˜ì •í•˜ì§€ ì•Šê³  êµ¬í˜„í•˜ê¸° ë•Œë¬¸ì— ì¶”í›„ì— ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì§€ ì•Šë„ë¡ ìµœëŒ€í•œ ì •í™•í•˜ê³  ì„¸ì„¸í•˜ê²Œ ì‘ì„±í•´ì•¼í•œë‹¤.
    ëª¨ë“  ê¸°ëŠ¥ì€ í™•ì •ë˜ì–´ì•¼ í•œë‹¤.

    [ëª©í‘œ]
    - ê³ ê°ì˜ ì¶”ìƒì  ìš”êµ¬ë¥¼ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ì •ì œí•œë‹¤.
    - ê¸°ëŠ¥ì /ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ë¥¼ ëª…í™•íˆ ë¶„ë¦¬í•˜ê³ , ëª¨í˜¸ì„±ì„ ì œê±°í•œë‹¤.
    - ì •ë§ í•„ìš”í•œ ì •ë³´ì— ëŒ€í•´ì„œë§Œ ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ë³´ì™„í•œë‹¤.

    [ì‘ì„± ê·œì¹™]
    - ê¸°ëŠ¥/ë¹„ê¸°ëŠ¥ ìš”êµ¬ëŠ” ë²ˆí˜¸ ëª©ë¡ í˜•íƒœë¡œ ì‘ì„±í•˜ë˜, ê° í•­ëª©ì€ ì¸¡ì • ê°€ëŠ¥í•˜ê±°ë‚˜ ê²€ì¦ ê°€ëŠ¥í•œ ìˆ˜ì¹˜/ì¡°ê±´ì„ í¬í•¨í•œë‹¤. ì˜ˆ: "ì‘ë‹µ ì‹œê°„ p95 â‰¤ 300ms", "ë™ì‹œ ì ‘ì† 5,000 ì‚¬ìš©ì ì§€ì›".
    - ëª¨í˜¸í•œ í‘œí˜„(ë¹ ë¥´ê²Œ, í¬ê²Œ, ì•ˆì •ì  ë“±)ì€ ê¸ˆì§€í•˜ê³ , êµ¬ì²´ì  ìˆ˜ì¹˜/ì¡°ê±´/ì‚¬ë¡€ë¡œ ëŒ€ì²´í•œë‹¤.
    - ë²”ìœ„(Scope), ê°€ì •(Assumptions), ì œì•½(Constraints)ì´ ì•”ì‹œë˜ì–´ ìˆìœ¼ë©´ ëª…ì‹œì ìœ¼ë¡œ ë“œëŸ¬ë‚´ê³  í•´ë‹¹ í•­ëª©ì— í†µí•©í•œë‹¤.
    - ì‚¬ìš©ìì˜ ìš”ì²­ì´ ì—†ì„ ê²½ìš°ì—ëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ ë°©ë²•ì„ ìš°ì„  ì±„íƒí•˜ì—¬ ì‘ì„±í•˜ë©°, ë¦¬ìŠ¤í¬ë¥¼ ìµœì†Œí™”í•œë‹¤.
    - ë³´ì•ˆ/ê°œì¸ì •ë³´, ë¡œê¹…/ëª¨ë‹ˆí„°ë§, ë°°í¬/ë¡¤ë°±, êµ­ì œí™”/í˜„ì§€í™”, ì ‘ê·¼ì„± ë“± ì¼ë°˜ì ìœ¼ë¡œ ëˆ„ë½ë˜ê¸° ì‰¬ìš´ ë¹„ê¸°ëŠ¥ í•­ëª©ì„ ìŠµê´€ì ìœ¼ë¡œ ì ê²€í•œë‹¤.
    - ëŒ€í™” ì´ë ¥ì˜ í•µì‹¬ ìš”êµ¬ì™€ ë¹„í•µì‹¬ ì¡ìŒì„ êµ¬ë¶„í•˜ì—¬, í•µì‹¬ë§Œ ë°˜ì˜í•œë‹¤.
    - í”¼ë“œë°±ì´ ìˆë‹¤ë©´ ì´ë¥¼ ë°˜ì˜í•˜ì—¬ ìš”êµ¬ì‚¬í•­ì„ ìˆ˜ì •í•˜ë¼.
    - ê¸°ëŠ¥ì— ëŒ€í•œ ìš”êµ¬ì‚¬í•­ì€ ìµœëŒ€í•œ ìì„¸í•˜ê²Œ ì‘ì„±í•˜ê³ , ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­ì€ ìµœëŒ€í•œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•œë‹¤.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆ ì§€ì¹¨ì„ ì—„ê²©íˆ ë”°ë¥¸ë‹¤. í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.


    í˜„ì¬ ê³„íšëœ ê¸°ëŠ¥: {functional_requirements}

    í”¼ë“œë°±: {feedback}
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì ì‘ë‹µ: {request}")
    ])

    chain = prompt | llm.with_structured_output(AnalysisResult)

    result = chain.invoke({
        "request": state["request"],    
        "feedback": state.get("feedback", ""), 
        "functional_requirements": state.get("functional_requirements", "ì•„ì§ ì •ì˜ë˜ì§€ ì•ŠìŒ"),
        "non_functional_requirements": state.get("non_functional_requirements", "ì•„ì§ ì •ì˜ë˜ì§€ ì•ŠìŒ"),
    })

    return {
        "goal": result.goal,
        "functional_requirements": result.functional_requirements, 
        "non_functional_requirements": result.non_functional_requirements
    }

def feedback_analysis(state: AgentState):
    """ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€í•˜ëŠ” ë…¸ë“œ"""

    print("--- ğŸ“ ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€ ì¤‘... ---")
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
   
    system_prompt = """
    ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê°€ì´ì ì†Œí”„íŠ¸ì›¨ì–´ ì•„í‚¤í…íŠ¸ë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì— ëŒ€í•´ ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì„ í‰ê°€í•˜ë¼.

    [í‰ê°€ ê·œì¹™]
    - ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì´ ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ì¶©ë¶„íˆ ë°˜ì˜í–ˆëŠ”ì§€ í‰ê°€í•˜ë¼.
    - ì¶©ë¶„íˆ ë°˜ì˜í–ˆë‹¤ë©´ is_completeë¥¼ trueë¡œ ì„¤ì •í•œë‹¤.
    - ì¶©ë¶„íˆ ë°˜ì˜í•˜ì§€ ëª»í–ˆë‹¤ë©´ is_completeë¥¼ falseë¡œ ì„¤ì •í•˜ê³  feedbackì— í‰ê°€ë¥¼ ì‘ì„±í•œë‹¤.
    
    - ë˜í•œ ì‚¬ìš©ìê°€ ìš”êµ¬ì‚¬í•­ì„ ì¶”í›„ì— ìˆ˜ì •í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ë¥¼ ê°ì•ˆí•˜ì—¬ í‰ê°€í•˜ë¼.

    [ì‚°ì¶œë¬¼ ê¸°ì¤€]
    - ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤.
    - ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì œê³µëœ JSON ìŠ¤í‚¤ë§ˆ ì§€ì¹¨ì„ ì—„ê²©íˆ ë”°ë¥¸ë‹¤. í•„ë“œ ì´ë¦„ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ì•ŠëŠ”ë‹¤.
      - feedback: ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ í‰ê°€.
      - is_complete: ì •ë³´ê°€ ì¶©ë¶„í•´ êµ¬í˜„ ê³„íš ìˆ˜ë¦½ì´ ê°€ëŠ¥í•˜ë©´ true, ì•„ë‹ˆë©´ false.

    ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­: {functional_requirements}
    ì‘ì„±ëœ ë¹„ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­: {non_functional_requirements}
  
    """

    chat_history = state["messages"]

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "ì‘ì„±ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ í‰ê°€í•˜ë¼."),
    ])

    chain = prompt | llm.with_structured_output(FeedbackAnalysisResult)
    result = chain.invoke({
        "chat_history": chat_history,
        "functional_requirements": state["functional_requirements"],
        "non_functional_requirements": state["non_functional_requirements"],
    })

    return {
        "feedback": result.feedback, 
        "is_complete": result.is_complete
    }
    

def is_complete(state: AgentState):
    return state["is_complete"]

def main(first_request: str):
    load_dotenv()

    workflow = StateGraph(AgentState)

    workflow.add_node("request_analysis", request_analysis)
    workflow.add_node("feedback_analysis", feedback_analysis)
    
    workflow.add_edge(START, "request_analysis") 
    workflow.add_edge("request_analysis", "feedback_analysis")
    workflow.add_conditional_edges("feedback_analysis", is_complete,{True: END, False: "request_analysis"})    

    app = workflow.compile()

    initial_state = {
        "request": first_request,
        "functional_requirements": "",
        "non_functional_requirements": "",
        "is_complete": False,
        "feedback": None,
        "messages": [HumanMessage(content=first_request)]
    }
    
    final_state = app.invoke(initial_state)

    print("\n" + "="*60)
    print("âœ… ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì™„ë£Œ!")
    print("="*60)
    print(f"\nğŸ“‹ ëª©í‘œ: {final_state['request']}")
    print(f"\nğŸ“Œ ê¸°ëŠ¥ì  ìš”êµ¬ì‚¬í•­:\n{final_state['functional_requirements']}")
    
    return final_state
    

if __name__ == "__main__":
    print("ğŸ’¡ ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”:")
    first_request = input("ğŸ‘‰ ")
    main(first_request)