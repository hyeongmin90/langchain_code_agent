import os
import json
from dotenv import load_dotenv
import operator
from typing import TypedDict, Annotated, List, Optional
from pydantic import BaseModel, Field
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, START, END
from schemas import DecomposeAgentState, UserStoriesResult, RefinedUserStoriesResult, FinalUserStoriesResult, RefinedUserStoriesDraft
import asyncio
import pprint

def decompose_into_user_stories(state: DecomposeAgentState):
    print("--- ğŸ“ 1ë‹¨ê³„ ê²°ê³¼ë¬¼ ìƒì„± ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ì˜ ë¸Œë ˆì¸ìŠ¤í† ë° ì„¸ì…˜ì„ ì´ë„ëŠ”, ì°½ì˜ì ì´ê³  ì•„ì´ë””ì–´ê°€ ë„˜ì¹˜ëŠ” ì• ìì¼ í”„ë¡œë•íŠ¸ ì˜¤ë„ˆ(Agile Product Owner)ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì£¼ëœ ì„ë¬´ëŠ” ì™„ë²½í•œ ê³„íšì´ ì•„ë‹ˆë¼, ê³ ê°ì˜ ìš”ì²­ ì‚¬í•­ì—ì„œ ë‚˜ì˜¨ ê°€ëŠ¥ì„±ì„ ë¹ ì§ì—†ì´ í¬ì°©í•˜ì—¬ ì‚¬ìš©ì ìŠ¤í† ë¦¬ì˜ "ì´ˆì•ˆ" ëª©ë¡ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]
    ì‚¬ìš©ìì˜ ìš”ì²­ì‚¬í•­ì„ ë°”íƒ•ìœ¼ë¡œ, í”„ë¡œì íŠ¸ì˜ ì „ì²´ì ì¸ ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ì—í”½(Epic) 1ê°œì™€, 
    ê·¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê¸°ëŠ¥ ì•„ì´ë””ì–´ë¥¼ ë‹´ì€ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡(User Story Drafts)ì„ ìƒì„±í•˜ë¼.

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    - ìµœìƒìœ„ì—ëŠ” epicê³¼ user_stories_draft ë‘ ê°œì˜ í‚¤ê°€ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
    - ê° ì‚¬ìš©ì ìŠ¤í† ë¦¬ëŠ” ê³ ìœ í•œ idì™€ í•¨ê»˜, í‘œì¤€ í˜•ì‹ì¸ "As a..., I want to..., so that..." êµ¬ì¡°ë¥¼ ë”°ë¼ as_a, i_want_to, so_that í‚¤ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì‹­ì‹œì˜¤.
    """
    
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì ìš”ì²­ì‚¬í•­: {user_request}")
    ])
    
    chain = prompt | llm.with_structured_output(UserStoriesResult)
    
    result = chain.invoke({
        "user_request": state["user_request"]
    })
    print("ì‘ì„±ëœ ìœ ì €ìŠ¤í† ë¦¬ -----------------")
    print(result)
    print("--------------------------------")

    return {
        "raw_user_stories": result
    }


def refine_user_stories(state: DecomposeAgentState):
    print("--- ğŸ“ 2ë‹¨ê³„ ê²°ê³¼ë¬¼ ìƒì„± ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ìˆ˜ë§ì€ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µìœ¼ë¡œ ì´ëˆ 20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì• ìì¼ í”„ë¡œë•íŠ¸ ì˜¤ë„ˆ(Senior Agile Product Owner)ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ íŠ¹ê¸°ëŠ” ì£¼ë‹ˆì–´ íŒ€ì›ì´ ë§Œë“  ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆì„ ë‚ ì¹´ë¡­ê²Œ ë¶„ì„í•˜ì—¬, ì—…ê³„ í‘œì¤€ì¸ INVEST ì›ì¹™ì— ë”°ë¼ ëª…í™•í•˜ê³ , ê°€ì¹˜ ìˆìœ¼ë©°, ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í† ë¦¬ë¡œ ì¬íƒ„ìƒì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]  
    ì£¼ì–´ì§€ëŠ” [ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡]ì„ ê²€í† í•˜ê³ , ë‹¤ìŒ [ì •ì œ ê·œì¹™]ì— ë”°ë¼ ìˆ˜ì • ë° ê°œì„ í•˜ì—¬ ì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ëª©ë¡(Refined User Stories)ì„ ìƒì„±í•˜ë¼.

    [ì •ì œ ê·œì¹™ (INVEST ì›ì¹™ ê¸°ë°˜)]
    - ë³‘í•© (Merge): ì„œë¡œ ì¤‘ë³µë˜ê±°ë‚˜ ì§€ë‚˜ì¹˜ê²Œ ìœ ì‚¬í•œ ìŠ¤í† ë¦¬ê°€ ìˆë‹¤ë©´, í•˜ë‚˜ì˜ ëª…í™•í•œ ìŠ¤í† ë¦¬ë¡œ ë³‘í•©í•˜ë¼.
    - ë¶„í•´ (Decompose): í•˜ë‚˜ì˜ ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì—¬ëŸ¬ ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆë‹¤ë©´(Epicì— ê°€ê¹ë‹¤ë©´), ë…ë¦½ì ìœ¼ë¡œ ê°œë°œ ê°€ëŠ¥í•œ ë” ì‘ì€ ìŠ¤í† ë¦¬ ì—¬ëŸ¬ ê°œë¡œ ë¶„í•´í•˜ë¼.
    - êµ¬ì²´í™” (Specify): "ì‰½ê²Œ", "ë¹ ë¥´ê²Œ", "ì˜"ê³¼ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ì„ ì‚¬ìš©ìê°€ ì²´ê°í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ í–‰ë™ì´ë‚˜ ê²°ê³¼ë¡œ ë°”ê¾¸ë¼. (ì˜ˆ: "ìƒí’ˆì„ ì‰½ê²Œ ì°¾ëŠ”ë‹¤" -> "ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒí’ˆì„ í•„í„°ë§í•œë‹¤")
    - ê°€ì¹˜ ë¶€ì—¬ (Add Value): ëª¨ë“  ìŠ¤í† ë¦¬ê°€ ìµœì¢… ì‚¬ìš©ìì—ê²Œ ëª…í™•í•œ ê°€ì¹˜(so_that)ë¥¼ ì œê³µí•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ë¶ˆë¶„ëª…í•˜ë‹¤ë©´ ê°€ì¹˜ë¥¼ ëª…í™•íˆ í•˜ë¼.
    - ìš°ì„ ìˆœìœ„ ë¶€ì—¬ (Prioritize): ê° ìŠ¤í† ë¦¬ê°€ í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ì„±ê³µ(MVP)ì— ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ íŒë‹¨í•˜ì—¬ priority (High, Medium, Low)ë¥¼ ë¶€ì—¬í•˜ë¼.

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    - ê° ì‚¬ìš©ì ìŠ¤í† ë¦¬ ê°ì²´ì—ëŠ” priority í•„ë“œê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.
    - IDëŠ” ìƒˆë¡œìš´ ê³ ìœ í•œ ê°’ìœ¼ë¡œ ë‹¤ì‹œ ë¶€ì—¬í•˜ë¼.
    - ìœ ì‚¬í•˜ê±°ë‚˜ ì—°ê´€ëœ ìŠ¤í† ë¦¬ëŠ” ê·¸ë£¹í™”í•˜ì—¬ ê·¸ë£¹í™”ëœ ìŠ¤í† ë¦¬ ëª©ë¡ìœ¼ë¡œ ìƒì„±í•˜ë¼.
    - ê·¸ë£¹ì€ 1-3ê°œì˜ ìŠ¤í† ë¦¬ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•œë‹¤.
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ìì˜ ìµœì´ˆ ìš”ì²­ ì‚¬í•­: {user_request}\nì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡:\n {user_stories_draft}")
    ])
    
    chain = prompt | llm.with_structured_output(RefinedUserStoriesResult)
    
    result = chain.invoke({
        "user_request": state["user_request"],
        "user_stories_draft": state["raw_user_stories"]
    })

    print("ì •ì œëœ ìœ ì €ìŠ¤í† ë¦¬ -----------------")
    for story_group in result.refined_user_stories:
        for story in story_group:
            print(story)
        print("\n--------------------------------")


    return {
        "epic": result.epic,
        "refined_user_stories": result.refined_user_stories
    }


async def generate_final_specifications(state: DecomposeAgentState):
    print("--- ğŸ“ 3ë‹¨ê³„ ê²°ê³¼ë¬¼ ìƒì„± ì¤‘... ---")
    
    LIMIT = 5
    semaphore = asyncio.Semaphore(LIMIT)
    tasks = [llm_call(i, semaphore, user_story_group, state["epic"]) for i, user_story_group in enumerate(state["refined_user_stories"])]

    results = await asyncio.gather(*tasks)

    flattened_stories = [item for sublist in results for item in sublist]

    final_result = {
        "epic": state["epic"],
        "final_user_stories": results
    }

    print("ìµœì¢… ê²°ê³¼ë¬¼ -----------------")
    for story in flattened_stories:
        print(story.model_dump_json(indent=2))
    print("--------------------------------")

    return {
        "final_specifications": final_result
    }

async def llm_call(task_id: int, semaphore: asyncio.Semaphore, user_story_group: List[RefinedUserStoriesDraft], epic: Epic):
    async with semaphore:
        print(f"Task {task_id} started")

        llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
        system_prompt = """
        [ì—­í• ]
        ë‹¹ì‹ ì€ ë””í…Œì¼ì— ê°•í•˜ê³  ë…¼ë¦¬ì ì¸ ì‚¬ê³ ë¥¼ í•˜ëŠ” QA(í’ˆì§ˆ ë³´ì¦) ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” í”„ë¡œë•íŠ¸ ì˜¤ë„ˆê°€ ì‘ì„±í•œ ì‚¬ìš©ì ìŠ¤í† ë¦¬ë¥¼ ë³´ê³ , ê°œë°œìê°€ ë¬´ì—‡ì„ ë§Œë“¤ì–´ì•¼ í•˜ê³  í…ŒìŠ¤í„°ê°€ ë¬´ì—‡ì„ ê²€ì¦í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆë„ë¡, êµ¬ì²´ì ì¸ **ìˆ˜ìš© ê¸°ì¤€(Acceptance Criteria)**ì„ ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

        [ì§€ì‹œ]
        ì£¼ì–´ì§„ ì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ê·¸ë£¹ì˜ ê°ê°ì— ëŒ€í•´, ê°œë°œ ì™„ë£Œ ì—¬ë¶€ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ìˆ˜ìš© ê¸°ì¤€ì„ 2ê°œ ì´ìƒ ìƒì„±í•˜ì‹­ì‹œì˜¤.

        [ìˆ˜ìš© ê¸°ì¤€ ì‘ì„± ê·œì¹™]
        - Gherkin í˜•ì‹ ì‚¬ìš©: ëª¨ë“  ìˆ˜ìš© ê¸°ì¤€ì€ "Given-When-Then" ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ë¼.
        - Given: ì‹œë‚˜ë¦¬ì˜¤ê°€ ì‹œì‘ë˜ê¸° ì „ì˜ ì „ì œ ì¡°ê±´
        - When: ì‚¬ìš©ìê°€ ì·¨í•˜ëŠ” íŠ¹ì • í–‰ë™
        - Then: ê·¸ í–‰ë™ìœ¼ë¡œ ì¸í•´ ë°œìƒí•´ì•¼ í•˜ëŠ” ê¸°ëŒ€ ê²°ê³¼
        - ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤: ê° ìŠ¤í† ë¦¬ì— ëŒ€í•´ ìµœì†Œ 1ê°œì˜ ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤("Happy Path")ì™€ 1ê°œ ì´ìƒì˜ ì‹¤íŒ¨ ë˜ëŠ” ì—£ì§€ ì¼€ì´ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤("Sad Path")ë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤.
        - êµ¬ì²´ì„±: "ì‚¬ìš©ì ì •ë³´ê°€ ë³´ì¸ë‹¤"ì™€ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ ëŒ€ì‹ , "í™”ë©´ ìƒë‹¨ì— ì‚¬ìš©ìì˜ ì´ë¦„ê³¼ ì´ë©”ì¼ ì£¼ì†Œê°€ í‘œì‹œëœë‹¤"ì²˜ëŸ¼ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•˜ê²Œ ì‘ì„±í•˜ë¼.

        [ì¶œë ¥ í˜•ì‹]
        - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
        - ì…ë ¥ë°›ì€ ìŠ¤í† ë¦¬ ì •ë³´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , acceptance_criteria ë¼ëŠ” í‚¤ë¥¼ ì¶”ê°€í•˜ë¼.
        - acceptance_criteriaëŠ” ê° ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹´ì€ ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•œë‹¤.
        - ê° ì‹œë‚˜ë¦¬ì˜¤ì—ëŠ” scenario í•„ë“œê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.
        """
        
        prompt = ChatPromptTemplate([
            ("system", system_prompt),  
            ("human", "ì—í”½: {epic}\nì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ê·¸ë£¹:\n {user_story_group}")
        ])

        chain = prompt | llm.with_structured_output(FinalUserStoriesResult)

        result = await chain.ainvoke({
            "user_story_group": user_story_group,
            "epic": epic
        })
        print(f"Task {task_id} completed-------")
        return result.final_user_stories


async def main(user_request: str):
    load_dotenv()

    workflow = StateGraph(DecomposeAgentState)

    workflow.add_node("decompose_into_user_stories", decompose_into_user_stories)
    workflow.add_node("refine_user_stories", refine_user_stories)
    workflow.add_node("generate_final_specifications", generate_final_specifications)

    workflow.add_edge(START, "decompose_into_user_stories")
    workflow.add_edge("decompose_into_user_stories", "refine_user_stories")
    workflow.add_edge("refine_user_stories", "generate_final_specifications")
    workflow.add_edge("generate_final_specifications", END)

    app = workflow.compile()

    initial_state = {
        "user_request": user_request,
        "epic": None,
        "raw_user_stories": None,
        "refined_user_stories": None,
        "final_specifications": None
    }

    final_state = await app.ainvoke(initial_state)

    return final_state["final_specifications"]

