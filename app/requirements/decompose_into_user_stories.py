import os
import json
from dotenv import load_dotenv
import operator
from typing import TypedDict, Annotated, List, Optional
from pydantic import BaseModel, Field
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, START, END


class AgentState(TypedDict):
    user_request: str
    raw_user_stories: Optional[user_stories_result]
    refined_user_stories: Optional[user_stories_result]
    final_specifications: Optional[final_user_stories_result]

class epic(BaseModel):
    title: str = Field(description="í”„ë¡œì íŠ¸ ì „ì²´ì˜ ìµœìƒìœ„ ëª©í‘œ ì œëª©")
    goal: str = Field(description="í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ìƒì„¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ")

class user_stories_draft(BaseModel):
    id: str = Field(description="ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ê³ ìœ  ì‹ë³„ì")
    as_a: str = Field(description="ìŠ¤í† ë¦¬ì˜ ì£¼ì²´ê°€ ë˜ëŠ” ì‚¬ìš©ì ì—­í• ")
    i_want_to: str = Field(description="ì‚¬ìš©ìê°€ ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ëª©í‘œë‚˜ í–‰ë™")
    so_that: str = Field(description="ê·¸ ëª©í‘œë¥¼ í†µí•´ ì–»ê²Œ ë˜ëŠ” ê°€ì¹˜ë‚˜ ì´ìœ ")

class user_stories_result(BaseModel):
    epic: epic = Field(description="í”„ë¡œì íŠ¸ì˜ ìµœìƒìœ„ ëª©í‘œë¥¼ ì •ì˜í•˜ëŠ” ì—í”½")
    user_stories_draft: List[user_stories_draft] = Field(description="ë¸Œë ˆì¸ìŠ¤í† ë°ì„ í†µí•´ ë„ì¶œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡")

class acceptance_criteria(BaseModel):
    scenario: str = Field(description="ì‹œë‚˜ë¦¬ì˜¤ ì œëª©")
    given: str = Field(description="ì‹œë‚˜ë¦¬ì˜¤ê°€ ì‹œì‘ë˜ê¸° ì „ì˜ ì „ì œ ì¡°ê±´")
    when: str = Field(description="ì‚¬ìš©ìê°€ ì·¨í•˜ëŠ” íŠ¹ì • í–‰ë™")
    then: str = Field(description="ê·¸ í–‰ë™ìœ¼ë¡œ ì¸í•´ ë°œìƒí•´ì•¼ í•˜ëŠ” ê¸°ëŒ€ ê²°ê³¼")

class final_user_stories_result(BaseModel):
    epic: epic = Field(description="í”„ë¡œì íŠ¸ì˜ ìµœìƒìœ„ ëª©í‘œë¥¼ ì •ì˜í•˜ëŠ” ì—í”½")
    refined_user_stories: List[user_stories_draft] = Field(description="ì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ëª©ë¡")
    acceptance_criteria: List[acceptance_criteria] = Field(description="ê° ì‚¬ìš©ì ìŠ¤í† ë¦¬ì— ëŒ€í•œ ìˆ˜ìš© ê¸°ì¤€")

def decompose_into_user_stories(state: AgentState):
    print("--- ğŸ“ 1ë‹¨ê³„ ê²°ê³¼ë¬¼ ìƒì„± ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ìƒˆë¡œìš´ í”„ë¡œì íŠ¸ì˜ ë¸Œë ˆì¸ìŠ¤í† ë° ì„¸ì…˜ì„ ì´ë„ëŠ”, ì°½ì˜ì ì´ê³  ì•„ì´ë””ì–´ê°€ ë„˜ì¹˜ëŠ” ì• ìì¼ í”„ë¡œë•íŠ¸ ì˜¤ë„ˆ(Agile Product Owner)ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ ì£¼ëœ ì„ë¬´ëŠ” ì™„ë²½í•œ ê³„íšì´ ì•„ë‹ˆë¼, ê³ ê°ì˜ ìš”ì²­ ì‚¬í•­ì—ì„œ ë‚˜ì˜¨ ê°€ëŠ¥ì„±ì„ ë¹ ì§ì—†ì´ í¬ì°©í•˜ì—¬ ì‚¬ìš©ì ìŠ¤í† ë¦¬ì˜ "ì´ˆì•ˆ" ëª©ë¡ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]
    ì•„ë˜ [ëŒ€í™” ë‚´ìš© ìš”ì•½]ì„ ë°”íƒ•ìœ¼ë¡œ, í”„ë¡œì íŠ¸ì˜ ì „ì²´ì ì¸ ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ì—í”½(Epic) 1ê°œì™€, 
    ê·¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê¸°ëŠ¥ ì•„ì´ë””ì–´ë¥¼ ë‹´ì€ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡(User Story Drafts)ì„ ìƒì„±í•˜ë¼.

    [ì‚¬ìš©ì ìš”ì²­ì‚¬í•­]
    {user_request}

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    - ìµœìƒìœ„ì—ëŠ” epicê³¼ user_stories_draft ë‘ ê°œì˜ í‚¤ê°€ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤.
    - ê° ì‚¬ìš©ì ìŠ¤í† ë¦¬ëŠ” ê³ ìœ í•œ idì™€ í•¨ê»˜, í‘œì¤€ í˜•ì‹ì¸ "As a..., I want to..., so that..." êµ¬ì¡°ë¥¼ ë”°ë¼ as_a, i_want_to, so_that í‚¤ë¡œ ë‚˜ëˆ„ì–´ ì‘ì„±í•˜ì‹­ì‹œì˜¤.

    """
    
    prompt = ChatPromptTemplate([
        ("system", system_prompt),
        ("human", "ì‚¬ìš©ì ì‘ë‹µ: {user_request}")
    ])
    
    chain = prompt | llm.with_structured_output(user_stories_result)
    
    result = chain.invoke({
        "user_request": state["user_request"]
    })
    print("ì‘ì„±ëœ ìœ ì €ìŠ¤í† ë¦¬ -----------------")
    print(result)
    print("--------------------------------")

    return {
        "raw_user_stories": result
    }



def refine_user_stories(state: AgentState):
    print("--- ğŸ“ 2ë‹¨ê³„ ê²°ê³¼ë¬¼ ìƒì„± ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ìˆ˜ë§ì€ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µìœ¼ë¡œ ì´ëˆ 20ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì• ìì¼ í”„ë¡œë•íŠ¸ ì˜¤ë„ˆ(Senior Agile Product Owner)ì…ë‹ˆë‹¤. 
    ë‹¹ì‹ ì˜ íŠ¹ê¸°ëŠ” ì£¼ë‹ˆì–´ íŒ€ì›ì´ ë§Œë“  ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆì„ ë‚ ì¹´ë¡­ê²Œ ë¶„ì„í•˜ì—¬, ì—…ê³„ í‘œì¤€ì¸ INVEST ì›ì¹™ì— ë”°ë¼ ëª…í™•í•˜ê³ , ê°€ì¹˜ ìˆìœ¼ë©°, ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í† ë¦¬ë¡œ ì¬íƒ„ìƒì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]  
    ì£¼ì–´ì§€ëŠ” [ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡]ì„ ê²€í† í•˜ê³ , ë‹¤ìŒ [ì •ì œ ê·œì¹™]ì— ë”°ë¼ ìˆ˜ì • ë° ê°œì„ í•˜ì—¬ ì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ëª©ë¡(Refined User Stories)ì„ ìƒì„±í•˜ë¼.

    [ì •ì œ ê·œì¹™ (INVEST ì›ì¹™ ê¸°ë°˜)]
    - ë³‘í•© (Merge): ì„œë¡œ ì¤‘ë³µë˜ê±°ë‚˜ ì§€ë‚˜ì¹˜ê²Œ ìœ ì‚¬í•œ ìŠ¤í† ë¦¬ê°€ ìˆë‹¤ë©´, í•˜ë‚˜ì˜ ëª…í™•í•œ ìŠ¤í† ë¦¬ë¡œ ë³‘í•©í•˜ë¼.
    - ë¶„í•´ (Decompose): í•˜ë‚˜ì˜ ìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì—¬ëŸ¬ ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆë‹¤ë©´(Epicì— ê°€ê¹ë‹¤ë©´), ë…ë¦½ì ìœ¼ë¡œ ê°œë°œ ê°€ëŠ¥í•œ ë” ì‘ì€ ìŠ¤í† ë¦¬ ì—¬ëŸ¬ ê°œë¡œ ë¶„í•´í•˜ë¼.
    - êµ¬ì²´í™” (Specify): "ì‰½ê²Œ", "ë¹ ë¥´ê²Œ", "ì˜"ê³¼ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ì„ ì‚¬ìš©ìê°€ ì²´ê°í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ í–‰ë™ì´ë‚˜ ê²°ê³¼ë¡œ ë°”ê¾¸ë¼. (ì˜ˆ: "ìƒí’ˆì„ ì‰½ê²Œ ì°¾ëŠ”ë‹¤" -> "ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìƒí’ˆì„ í•„í„°ë§í•œë‹¤")
    - ê°€ì¹˜ ë¶€ì—¬ (Add Value): ëª¨ë“  ìŠ¤í† ë¦¬ê°€ ìµœì¢… ì‚¬ìš©ìì—ê²Œ ëª…í™•í•œ ê°€ì¹˜(so_that)ë¥¼ ì œê³µí•˜ëŠ”ì§€ í™•ì¸í•˜ê³ , ë¶ˆë¶„ëª…í•˜ë‹¤ë©´ ê°€ì¹˜ë¥¼ ëª…í™•íˆ í•˜ë¼.
    - ìš°ì„ ìˆœìœ„ ë¶€ì—¬ (Prioritize): ê° ìŠ¤í† ë¦¬ê°€ í”„ë¡œì íŠ¸ì˜ í•µì‹¬ ì„±ê³µ(MVP)ì— ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ íŒë‹¨í•˜ì—¬ priority (High, Medium, Low)ë¥¼ ë¶€ì—¬í•˜ë¼.

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    - epic ì •ë³´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , user_stories_draft í‚¤ë¥¼ refined_user_storiesë¡œ ë³€ê²½í•˜ë¼.
    - ê° ì‚¬ìš©ì ìŠ¤í† ë¦¬ ê°ì²´ì—ëŠ” priority í•„ë“œê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.
    - IDëŠ” STORY-001, STORY-002 ì™€ ê°™ì´ ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ì‹œ ë¶€ì—¬í•˜ë¼.
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt)
        ("human", "ì‚¬ìš©ìì˜ ìµœì´ˆ ìš”ì²­ ì‚¬í•­: {user_request}\nì‚¬ìš©ì ìŠ¤í† ë¦¬ ì´ˆì•ˆ ëª©ë¡:\n {user_stories_draft}")
    ])
    
    chain = prompt | llm.with_structured_output(user_stories_result)
    
    result = chain.invoke({
        "user_request": state["user_request"],
        "user_stories_draft": state["raw_user_stories"]
    })

    print("ì •ì œëœ ìœ ì €ìŠ¤í† ë¦¬ -----------------")
    print(result)
    print("--------------------------------")

    return {
        "refined_user_stories": result
    }

def generate_final_specifications(state: AgentState):
    print("--- ğŸ“ 3ë‹¨ê³„ ê²°ê³¼ë¬¼ ìƒì„± ì¤‘... ---")

    llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")
    system_prompt = """
    [ì—­í• ]
    ë‹¹ì‹ ì€ ë””í…Œì¼ì— ê°•í•˜ê³  ë…¼ë¦¬ì ì¸ ì‚¬ê³ ë¥¼ í•˜ëŠ” QA(í’ˆì§ˆ ë³´ì¦) ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” í”„ë¡œë•íŠ¸ ì˜¤ë„ˆê°€ ì‘ì„±í•œ ì‚¬ìš©ì ìŠ¤í† ë¦¬ë¥¼ ë³´ê³ , ê°œë°œìê°€ ë¬´ì—‡ì„ ë§Œë“¤ì–´ì•¼ í•˜ê³  í…ŒìŠ¤í„°ê°€ ë¬´ì—‡ì„ ê²€ì¦í•´ì•¼ í•˜ëŠ”ì§€ ëª…í™•íˆ ì•Œ ìˆ˜ ìˆë„ë¡, êµ¬ì²´ì ì¸ **ìˆ˜ìš© ê¸°ì¤€(Acceptance Criteria)**ì„ ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

    [ì§€ì‹œ]
    ì£¼ì–´ì§„ ì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ê°ê°ì— ëŒ€í•´, ê°œë°œ ì™„ë£Œ ì—¬ë¶€ë¥¼ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ìˆ˜ìš© ê¸°ì¤€ì„ 2ê°œ ì´ìƒ ìƒì„±í•˜ì‹­ì‹œì˜¤.

    [ìˆ˜ìš© ê¸°ì¤€ ì‘ì„± ê·œì¹™]
    - Gherkin í˜•ì‹ ì‚¬ìš©: ëª¨ë“  ìˆ˜ìš© ê¸°ì¤€ì€ "Given-When-Then" ì‹œë‚˜ë¦¬ì˜¤ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ë¼.
    - Given: ì‹œë‚˜ë¦¬ì˜¤ê°€ ì‹œì‘ë˜ê¸° ì „ì˜ ì „ì œ ì¡°ê±´
    - When: ì‚¬ìš©ìê°€ ì·¨í•˜ëŠ” íŠ¹ì • í–‰ë™
    - Then: ê·¸ í–‰ë™ìœ¼ë¡œ ì¸í•´ ë°œìƒí•´ì•¼ í•˜ëŠ” ê¸°ëŒ€ ê²°ê³¼
    - ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤: ê° ìŠ¤í† ë¦¬ì— ëŒ€í•´ ìµœì†Œ 1ê°œì˜ ì„±ê³µ ì‹œë‚˜ë¦¬ì˜¤("Happy Path")ì™€ 1ê°œ ì´ìƒì˜ ì‹¤íŒ¨ ë˜ëŠ” ì—£ì§€ ì¼€ì´ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤("Sad Path")ë¥¼ í¬í•¨í•´ì•¼ í•œë‹¤.
    - êµ¬ì²´ì„±: "ì‚¬ìš©ì ì •ë³´ê°€ ë³´ì¸ë‹¤"ì™€ ê°™ì€ ëª¨í˜¸í•œ í‘œí˜„ ëŒ€ì‹ , "í™”ë©´ ìƒë‹¨ì— ì‚¬ìš©ìì˜ ì´ë¦„ê³¼ ì´ë©”ì¼ ì£¼ì†Œê°€ í‘œì‹œëœë‹¤"ì²˜ëŸ¼ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•˜ê²Œ ì‘ì„±í•˜ë¼.

    [ì¶œë ¥ í˜•ì‹]
    - ë°˜ë“œì‹œ ìœ íš¨í•œ pydantic ëª¨ë¸ì˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼.
    - ì…ë ¥ë°›ì€ ìŠ¤í† ë¦¬ ì •ë³´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , acceptance_criteria ë¼ëŠ” í‚¤ë¥¼ ì¶”ê°€í•˜ë¼.
    - acceptance_criteriaëŠ” ê° ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‹´ì€ ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•œë‹¤.
    - ê° ì‹œë‚˜ë¦¬ì˜¤ì—ëŠ” scenario í•„ë“œê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.
    """

    prompt = ChatPromptTemplate([
        ("system", system_prompt)
        ("human", "ì‚¬ìš©ìì˜ ìµœì´ˆ ìš”ì²­ ì‚¬í•­:\n {user_request}\nì •ì œëœ ì‚¬ìš©ì ìŠ¤í† ë¦¬:\n {refined_user_stories}")
    ])
    
    chain = prompt | llm.with_structured_output(final_user_stories_result)
    
    result = chain.invoke({
        "user_request": state["user_request"],
        "refined_user_stories": state["refined_user_stories"]
    })

    print("ìµœì¢… ìˆ˜ìš© ê¸°ì¤€ -----------------")
    print(result)
    print("--------------------------------")

    return {
        "final_specifications": result
    }


def main(user_request: str):
    load_dotenv()

    workflow = StateGraph(AgentState)

    workflow.add_node("decompose_into_user_stories", decompose_into_user_stories)
    workflow.add_node("refine_user_stories", refine_user_stories)
    workflow.add_node("generate_final_specifications", generate_final_specifications)

    workflow.add_edge(START, "decompose_into_user_stories")
    workflow.add_edge("decompose_into_user_stories", "refine_user_stories")
    workflow.add_edge("refine_user_stories", "generate_final_specifications")
    workflow.add_edge("generate_final_specifications", END)

    app = workflow.compile()

    initial_state = {
        "user_request": user_request,
        "raw_user_stories": None,
        "refined_user_stories": None,
        "final_specifications": None
    }

    final_state = app.invoke(initial_state)

    return final_state["final_specifications"]